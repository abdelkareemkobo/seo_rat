{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp semantic_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# | export\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, PointStruct, Distance\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from uuid import uuid4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# | export\n",
    "class KeywordVectorDB:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2\",\n",
    "    ):\n",
    "        self.model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "        self.client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "    def _get_collection_name(self, page_url: str) -> str:\n",
    "        \"\"\"Convert URL to valid collection name\"\"\"\n",
    "        return page_url.replace(\"https://\", \"\").replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "    def get_or_create_collection(self, page_url: str) -> str:\n",
    "        \"\"\"Get existing collection or create if doesn't exist\"\"\"\n",
    "        collection_name = self._get_collection_name(page_url)\n",
    "        collections = self.client.get_collections().collections\n",
    "        exists = any(c.name == collection_name for c in collections)\n",
    "\n",
    "        if not exists:\n",
    "            self.client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.model.get_sentence_embedding_dimension(),\n",
    "                    distance=Distance.COSINE,\n",
    "                ),\n",
    "            )\n",
    "        return collection_name\n",
    "\n",
    "    def store_keywords(self, page_url: str, keywords_data: List[Dict]):\n",
    "        \"\"\"Store keywords with performance data\"\"\"\n",
    "        collection_name = self.get_or_create_collection(page_url)\n",
    "        points = []\n",
    "\n",
    "        for kw_data in keywords_data:\n",
    "            keyword = kw_data[\"keyword\"]\n",
    "            vector = self.model.encode(keyword)\n",
    "\n",
    "            point = PointStruct(\n",
    "                id=str(uuid4()),\n",
    "                vector=vector.tolist(),\n",
    "                payload={\n",
    "                    \"keyword\": keyword,\n",
    "                    \"clicks\": kw_data.get(\"clicks\", 0),\n",
    "                    \"impressions\": kw_data.get(\"impressions\", 0),\n",
    "                    \"position\": kw_data.get(\"position\", 0),\n",
    "                    \"ctr\": kw_data.get(\"ctr\", 0),\n",
    "                    \"in_content\": False,\n",
    "                    \"is_important\": False,\n",
    "                    \"last_updated\": datetime.now().isoformat(),\n",
    "                },\n",
    "            )\n",
    "            points.append(point)\n",
    "\n",
    "        if points:\n",
    "            self.client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "    def search_keywords(\n",
    "        self, page_url: str, query_text: str, limit: int = 10\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Search similar keywords\"\"\"\n",
    "        collection_name = self._get_collection_name(page_url)\n",
    "        query_vector = self.model.encode(query_text)\n",
    "\n",
    "        results = self.client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector.tolist(),\n",
    "            limit=limit,\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            {\"keyword\": r.payload[\"keyword\"], \"score\": r.score, **r.payload}\n",
    "            for r in results\n",
    "        ]\n",
    "\n",
    "    def get_keywords(self, page_url: str, min_clicks: int = None) -> List[Dict]:\n",
    "        \"\"\"Get all keywords for a page\"\"\"\n",
    "        collection_name = self._get_collection_name(page_url)\n",
    "        results = self.client.scroll(collection_name=collection_name, limit=1000)[0]\n",
    "\n",
    "        keywords = [{\"keyword\": r.payload[\"keyword\"], **r.payload} for r in results]\n",
    "\n",
    "        if min_clicks:\n",
    "            keywords = [k for k in keywords if k.get(\"clicks\", 0) >= min_clicks]\n",
    "\n",
    "        return keywords\n",
    "\n",
    "    def update_content_status(self, page_url: str, content: str):\n",
    "        \"\"\"Mark which keywords appear in content\"\"\"\n",
    "        collection_name = self._get_collection_name(page_url)\n",
    "        results = self.client.scroll(collection_name=collection_name, limit=1000)[0]\n",
    "\n",
    "        for point in results:\n",
    "            keyword = point.payload[\"keyword\"]\n",
    "            in_content = keyword.lower() in content.lower()\n",
    "\n",
    "            self.client.set_payload(\n",
    "                collection_name=collection_name,\n",
    "                payload={\n",
    "                    \"in_content\": in_content,\n",
    "                    \"last_updated\": datetime.now().isoformat(),\n",
    "                },\n",
    "                points=[point.id],\n",
    "            )\n",
    "    def set_keyword_importance(self, page_url: str, keyword: str, is_important: bool):\n",
    "        \"\"\"Mark keyword as important or not\"\"\"\n",
    "        collection_name = self._get_collection_name(page_url)\n",
    "\n",
    "        results = self.client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            scroll_filter={\"must\": [{\"key\": \"keyword\", \"match\": {\"value\": keyword}}]},\n",
    "            limit=1,\n",
    "        )[0]\n",
    "\n",
    "        if results:\n",
    "            point = results[0]\n",
    "            self.client.set_payload(\n",
    "                collection_name=collection_name,\n",
    "                payload={\n",
    "                    \"is_important\": is_important,\n",
    "                    \"last_updated\": datetime.now().isoformat(),\n",
    "                },\n",
    "                points=[point.id],\n",
    "            )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def get_keyword_importance(self, page_url: str, keyword: str) -> Optional[bool]:\n",
    "        \"\"\"Check if keyword is marked important\"\"\"\n",
    "        collection_name = self._get_collection_name(page_url)\n",
    "\n",
    "        results = self.client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            scroll_filter={\"must\": [{\"key\": \"keyword\", \"match\": {\"value\": keyword}}]},\n",
    "            limit=1,\n",
    "        )[0]\n",
    "\n",
    "        return results[0].payload.get(\"is_important\") if results else None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
