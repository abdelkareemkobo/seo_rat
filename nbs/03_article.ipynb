{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime, date\n",
    "from typing import Optional, Dict, Any, List\n",
    "from pathlib import Path\n",
    "from pymongo.database import Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seo_rat.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Article:\n",
    "    \"\"\"Class representing an article's metadata and analysis\"\"\"\n",
    "\n",
    "    # File information (only required fields)\n",
    "    relative_path: str  # Path relative to content directory\n",
    "    website_domain: str\n",
    "    base_path: str\n",
    "\n",
    "    # Frontmatter data (will be populated from file)\n",
    "    title: Optional[str] = None\n",
    "    publishDate: Optional[str] = None\n",
    "    excerpt: Optional[str] = None\n",
    "    image: Optional[str] = None\n",
    "    tags: Optional[List[str]] = None\n",
    "    # Focus keyword\n",
    "    focus_keyword: Optional[str] = None\n",
    "    related_keywords: List[str] = None\n",
    "\n",
    "    # Analysis data (will be populated from content)\n",
    "    headers: List[Dict[str, Any]] = None\n",
    "    internal_links: Dict[str, Dict] = None\n",
    "    external_links: Dict[str, Dict] = None\n",
    "    images_data: List[Dict[str, str]] = None\n",
    "    phone_numbers: List[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Load and analyze file content\"\"\"\n",
    "        self.load_from_file()\n",
    "\n",
    "    def get_full_path(self) -> str:\n",
    "        \"\"\"Get full file path\"\"\"\n",
    "        return str(Path(self.base_path) / self.relative_path)\n",
    "\n",
    "    def load_from_file(self):\n",
    "        \"\"\"Load and parse file content\"\"\"\n",
    "        with open(self.get_full_path(), \"r\") as f:\n",
    "            content = f.read()\n",
    "        # Extract frontmatter\n",
    "        self.title, self.publishDate, self.excerpt, self.image, self.tags = (\n",
    "            extract_frontmatter(content)\n",
    "        )\n",
    "\n",
    "        # Get content without frontmatter\n",
    "        main_content = exclude_front_matter(content)\n",
    "\n",
    "        # Analyze content\n",
    "        self.analyze_content(main_content)\n",
    "\n",
    "    def analyze_content(self, content: str):\n",
    "        \"\"\"Analyze the content and populate metadata\"\"\"\n",
    "        self.headers = get_heads_info(self.get_full_path())  # Changed from file_path\n",
    "\n",
    "        urls_data = extract_markdown_urls_with_tags(content)\n",
    "        self.internal_links = {\n",
    "            url: data\n",
    "            for url, data in urls_data.items()\n",
    "            if url in get_internal_urls([url], self.website_domain)\n",
    "        }\n",
    "        self.external_links = {\n",
    "            url: data\n",
    "            for url, data in urls_data.items()\n",
    "            if url in get_external_urls([url], self.website_domain)\n",
    "        }\n",
    "        self.images_data = extract_markdown_images(content)\n",
    "        self.phone_numbers = detect_numbers(content)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert article data to MongoDB document format\"\"\"\n",
    "        # Convert publishDate to ISO format string if it's a datetime\n",
    "        publish_date = self.publishDate\n",
    "        if isinstance(publish_date, (datetime, date)):\n",
    "            publish_date = publish_date.isoformat()\n",
    "\n",
    "        return {\n",
    "            \"relative_path\": self.relative_path,  # Changed from file_path\n",
    "            \"base_path\": self.base_path,\n",
    "            \"website_domain\": self.website_domain,\n",
    "            \"focus_keyword\": self.focus_keyword,\n",
    "            \"metadata\": {\n",
    "                \"title\": self.title,\n",
    "                \"publishDate\": publish_date,  # Use converted date\n",
    "                \"excerpt\": self.excerpt,\n",
    "                \"image\": self.image,\n",
    "                \"tags\": self.tags,\n",
    "            },\n",
    "            \"analysis\": {\n",
    "                \"headers\": self.headers,\n",
    "                \"internal_links\": self.internal_links,\n",
    "                \"external_links\": self.external_links,\n",
    "                \"images\": self.images_data,\n",
    "                \"phone_numbers\": self.phone_numbers,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "class ArticleStore:\n",
    "    \"\"\"Handle MongoDB operations for Article objects\"\"\"\n",
    "\n",
    "    def __init__(self, db: Database, content_base_path: str):\n",
    "        self.db = db\n",
    "        self.collection = self.db.articles\n",
    "        self.content_base_path = Path(content_base_path)\n",
    "\n",
    "    def check_duplicate_before_insert(\n",
    "        self, article: Article, similarity_threshold: float = 0.8\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Check if article content is duplicate before insertion\"\"\"\n",
    "        current_content = exclude_front_matter(\n",
    "            open(article.get_full_path(), \"r\").read()\n",
    "        )\n",
    "        current_content = re.sub(r\"\\s+\", \" \", current_content).strip()\n",
    "\n",
    "        # Check existing articles\n",
    "        existing_articles = self.collection.find()\n",
    "        duplicates = []\n",
    "\n",
    "        for existing in existing_articles:\n",
    "            try:\n",
    "                # Skip if it's the same article (same website and path)\n",
    "                if (\n",
    "                    existing.get(\"website_domain\") == article.website_domain\n",
    "                    and existing.get(\"relative_path\") == article.relative_path\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                # Get existing article content\n",
    "                if \"relative_path\" in existing:\n",
    "                    existing_path = self.get_full_path(existing[\"relative_path\"])\n",
    "                else:\n",
    "                    existing_path = existing[\"file_path\"]\n",
    "\n",
    "                existing_content = exclude_front_matter(open(existing_path, \"r\").read())\n",
    "                existing_content = re.sub(r\"\\s+\", \" \", existing_content).strip()\n",
    "\n",
    "                # Calculate similarity\n",
    "                similarity = SequenceMatcher(\n",
    "                    None, current_content, existing_content\n",
    "                ).ratio()\n",
    "\n",
    "                if similarity >= similarity_threshold:\n",
    "                    duplicates.append(\n",
    "                        {\n",
    "                            \"title\": existing[\"metadata\"][\"title\"],\n",
    "                            \"similarity\": similarity,\n",
    "                            \"path\": existing.get(\"relative_path\")\n",
    "                            or existing.get(\"file_path\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: Error checking article {existing.get('relative_path', 'unknown')}: {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        return {\"has_duplicates\": bool(duplicates), \"duplicates\": duplicates}\n",
    "\n",
    "    def insert_or_update_article(self, article: Article) -> str:\n",
    "        \"\"\"Insert article if not exists, or update if exists based on website and path\"\"\"\n",
    "        # First check for duplicates\n",
    "        duplicate_check = self.check_duplicate_before_insert(article)\n",
    "        if duplicate_check[\"has_duplicates\"]:\n",
    "            print(\"Warning: Similar content detected:\")\n",
    "            for dup in duplicate_check[\"duplicates\"]:\n",
    "                print(f\"- {dup['title']} (similarity: {dup['similarity']:.2f})\")\n",
    "\n",
    "        article_dict = article.to_dict()\n",
    "\n",
    "        # Try to update existing article\n",
    "        result = self.collection.update_one(\n",
    "            {\n",
    "                \"website_domain\": article.website_domain,\n",
    "                \"relative_path\": article.relative_path,\n",
    "            },\n",
    "            {\"$set\": article_dict},\n",
    "            upsert=True,\n",
    "        )\n",
    "\n",
    "        if result.upserted_id:\n",
    "            return str(result.upserted_id)\n",
    "        else:\n",
    "            # Get the _id of the existing document\n",
    "            existing = self.collection.find_one(\n",
    "                {\n",
    "                    \"website_domain\": article.website_domain,\n",
    "                    \"relative_path\": article.relative_path,\n",
    "                }\n",
    "            )\n",
    "            return str(existing[\"_id\"])\n",
    "\n",
    "    def get_all_article_files(self) -> List[str]:\n",
    "        \"\"\"Get all markdown files in content directory\"\"\"\n",
    "        pattern = str(self.content_base_path / \"**/*.md\")\n",
    "        files = glob.glob(pattern, recursive=True)\n",
    "        return [str(Path(f).relative_to(self.content_base_path)) for f in files]\n",
    "\n",
    "    def find_by_tag(self, tag: str) -> List[Dict]:\n",
    "        \"\"\"Find articles by tag\"\"\"\n",
    "        return list(self.collection.find({\"metadata.tags\": tag}))\n",
    "\n",
    "    def find_articles_with_phone_numbers(self) -> List[Dict]:\n",
    "        \"\"\"Find articles containing phone numbers\"\"\"\n",
    "        return list(\n",
    "            self.collection.find(\n",
    "                {\"analysis.phone_numbers\": {\"$exists\": True, \"$ne\": []}}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_full_path(self, relative_path: str) -> str:\n",
    "        \"\"\"Convert relative path to full path\"\"\"\n",
    "        return str(self.content_base_path / relative_path)\n",
    "\n",
    "    def find_by_domain(self, domain: str) -> List[Dict]:\n",
    "        \"\"\"Find all articles for a specific website domain\"\"\"\n",
    "        return list(self.collection.find({\"website_domain\": domain}))\n",
    "\n",
    "    def search_in_title(self, keyword: str) -> List[Dict]:\n",
    "        \"\"\"Search for keyword in article titles\"\"\"\n",
    "        return list(\n",
    "            self.collection.find(\n",
    "                {\"metadata.title\": {\"$regex\": keyword, \"$options\": \"i\"}}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def set_article_keywords(\n",
    "        self, relative_path: str, focus_keyword: str, related_keywords: List[str] = None\n",
    "    ) -> bool:\n",
    "        \"\"\"Set focus keyword and related keywords for an article\"\"\"\n",
    "        if related_keywords is None:\n",
    "            related_keywords = []\n",
    "        result = self.collection.update_one(\n",
    "            {\"relative_path\": relative_path},\n",
    "            {\n",
    "                \"$set\": {\n",
    "                    \"focus_keyword\": focus_keyword,\n",
    "                    \"related_keywords\": related_keywords,\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        return result.modified_count > 0\n",
    "\n",
    "    def get_article_metadata(self, relative_path: str) -> Dict:\n",
    "        \"\"\"Get complete metadata for a specific article\"\"\"\n",
    "        article = self.collection.find_one({\"relative_path\": relative_path})\n",
    "        if article is None:\n",
    "            raise ValueError(f\"No article found with relative_path: {relative_path}\")\n",
    "        return article\n",
    "\n",
    "    def get_article_analysis(self, relative_path: str) -> Dict:\n",
    "        \"\"\"Get just the analysis part of an article\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        return article.get(\"analysis\", {})\n",
    "\n",
    "    def get_article_headers(self, relative_path: str) -> List[Dict]:\n",
    "        \"\"\"Get headers from specific article\"\"\"\n",
    "        analysis = self.get_article_analysis(relative_path)\n",
    "        return analysis.get(\"headers\", [])\n",
    "\n",
    "    def get_article_links(self, relative_path: str) -> Dict:\n",
    "        \"\"\"Get all links from specific article\"\"\"\n",
    "        analysis = self.get_article_analysis(relative_path)\n",
    "        return {\n",
    "            \"internal\": analysis.get(\"internal_links\", {}),\n",
    "            \"external\": analysis.get(\"external_links\", {}),\n",
    "        }\n",
    "\n",
    "    def get_article_phone_numbers(self, relative_path: str) -> List[str]:\n",
    "        \"\"\"Get phone numbers from specific article\"\"\"\n",
    "        analysis = self.get_article_analysis(relative_path)\n",
    "        return analysis.get(\"phone_numbers\", [])\n",
    "\n",
    "    def get_article_images(self, relative_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get images data from specific article\"\"\"\n",
    "        analysis = self.get_article_analysis(relative_path)\n",
    "        return analysis.get(\"images\", [])\n",
    "\n",
    "    def analyze_header_hierarchy(self, relative_path: str) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Analyze headers by level\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        headers = article[\"analysis\"][\"headers\"]\n",
    "        return {\n",
    "            \"h1\": [h for h in headers if h[\"type\"] == \"h1\"],\n",
    "            \"h2\": [h for h in headers if h[\"type\"] == \"h2\"],\n",
    "            \"h3\": [h for h in headers if h[\"type\"] == \"h3\"],\n",
    "            \"h4\": [h for h in headers if h[\"type\"] == \"h4\"],\n",
    "            \"h5\": [h for h in headers if h[\"type\"] == \"h5\"],\n",
    "            \"h6\": [h for h in headers if h[\"type\"] == \"h6\"],\n",
    "        }\n",
    "\n",
    "    def calculate_keyword_density(self, relative_path: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate keyword density for focus and related keywords\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        keywords = self.get_article_keywords(relative_path)\n",
    "        full_path = self.get_full_path(relative_path)\n",
    "        content = exclude_front_matter(open(full_path, \"r\").read())\n",
    "\n",
    "        # Convert content to lowercase for case-insensitive matching\n",
    "        content_lower = content.lower()\n",
    "        # Get total word count\n",
    "        total_words = len(content.split())\n",
    "\n",
    "        densities = {}\n",
    "\n",
    "        # Calculate focus keyword density\n",
    "        if keywords.get(\"focus_keyword\"):\n",
    "            focus_kw = keywords[\"focus_keyword\"].lower()\n",
    "            focus_count = content_lower.count(focus_kw)\n",
    "            densities[\"focus_keyword\"] = {\n",
    "                \"keyword\": keywords[\"focus_keyword\"],\n",
    "                \"count\": focus_count,\n",
    "                \"density\": (focus_count / total_words) * 100 if total_words > 0 else 0,\n",
    "            }\n",
    "\n",
    "        # Calculate related keywords density\n",
    "        densities[\"related_keywords\"] = []\n",
    "        for kw in keywords.get(\"related_keywords\", []):\n",
    "            kw_lower = kw.lower()\n",
    "            count = content_lower.count(kw_lower)\n",
    "            densities[\"related_keywords\"].append(\n",
    "                {\n",
    "                    \"keyword\": kw,\n",
    "                    \"count\": count,\n",
    "                    \"density\": (count / total_words) * 100 if total_words > 0 else 0,\n",
    "                }\n",
    "            )\n",
    "        self.collection.update_one(\n",
    "            {\"relative_path\": relative_path}, {\"$set\": {\"keyword_density\": densities}}\n",
    "        )\n",
    "        return densities\n",
    "\n",
    "    def get_article_keywords(self, relative_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get keywords data for an article\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        return {\n",
    "            \"focus_keyword\": article.get(\"focus_keyword\"),\n",
    "            \"related_keywords\": article.get(\"related_keywords\", []),\n",
    "        }\n",
    "\n",
    "    def analyze_seo_rules(self, relative_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze SEO rules for a specific article\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        keywords = self.get_article_keywords(relative_path)\n",
    "        full_path = self.get_full_path(relative_path)\n",
    "        focus_keyword = keywords.get(\"focus_keyword\")\n",
    "        related_keywords = keywords.get(\"related_keywords\", [])\n",
    "\n",
    "        # Get all headers of type h1\n",
    "        h1_headers = [h for h in article[\"analysis\"][\"headers\"] if h[\"type\"] == \"h1\"]\n",
    "\n",
    "        # Check if focus keyword is in excerpt\n",
    "        excerpt = article[\"metadata\"].get(\"excerpt\", \"\")\n",
    "\n",
    "        # Get content for 10% analysis\n",
    "        content = exclude_front_matter(open(full_path, \"r\").read())\n",
    "        first_10_percent = content[: int(len(content) * 0.1)]\n",
    "\n",
    "        # Get image alt texts\n",
    "        alt_texts = [img[\"alt_text\"] for img in article[\"analysis\"][\"images\"]]\n",
    "\n",
    "        # Check if focus keyword is unique across all articles\n",
    "        other_articles = list(\n",
    "            self.collection.find(\n",
    "                {\n",
    "                    \"relative_path\": {\"$ne\": relative_path},\n",
    "                    \"focus_keyword\": focus_keyword,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Count and find positions of focus keyword\n",
    "        keyword_positions = []\n",
    "        content_lower = content.lower()\n",
    "        focus_keyword_lower = focus_keyword.lower() if focus_keyword else \"\"\n",
    "        pos = 0\n",
    "        while (\n",
    "            focus_keyword\n",
    "            and (pos := content_lower.find(focus_keyword_lower, pos)) != -1\n",
    "        ):\n",
    "            keyword_positions.append(pos)\n",
    "            pos += 1\n",
    "\n",
    "        analysis = {\n",
    "            \"h1_count\": len(h1_headers),\n",
    "            \"has_single_h1\": len(h1_headers) == 1,\n",
    "            \"focus_keyword_in_excerpt\": (\n",
    "                focus_keyword and focus_keyword.lower() in excerpt.lower()\n",
    "                if excerpt\n",
    "                else False\n",
    "            ),\n",
    "            \"keywords_in_first_10_percent\": {\n",
    "                \"focus_keyword\": focus_keyword\n",
    "                and focus_keyword.lower() in first_10_percent.lower(),\n",
    "                \"related_keywords\": [\n",
    "                    kw\n",
    "                    for kw in related_keywords\n",
    "                    if kw.lower() in first_10_percent.lower()\n",
    "                ],\n",
    "            },\n",
    "            \"keywords_in_alt_text\": {\n",
    "                \"focus_keyword\": any(\n",
    "                    focus_keyword and focus_keyword.lower() in alt.lower()\n",
    "                    for alt in alt_texts\n",
    "                ),\n",
    "                \"related_keywords\": [\n",
    "                    kw\n",
    "                    for kw in related_keywords\n",
    "                    if any(kw.lower() in alt.lower() for alt in alt_texts)\n",
    "                ],\n",
    "            },\n",
    "            \"has_external_links\": bool(article[\"analysis\"][\"external_links\"]),\n",
    "            \"focus_keyword_is_unique\": len(other_articles) == 0,  # Changed this line\n",
    "            \"focus_keyword_stats\": {\n",
    "                \"count\": len(keyword_positions),\n",
    "                \"positions\": keyword_positions,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Update article with analysis\n",
    "        self.collection.update_one(\n",
    "            {\"relative_path\": relative_path}, {\"$set\": {\"seo_analysis\": analysis}}\n",
    "        )\n",
    "        return analysis\n",
    "\n",
    "    def detect_duplicate_content(\n",
    "        self, relative_path: str, similarity_threshold: float = 0.8\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Detect similar content across articles\"\"\"\n",
    "        current_article = self.get_article_metadata(relative_path)\n",
    "        full_path = self.get_full_path(relative_path)\n",
    "        current_content = exclude_front_matter(open(full_path, \"r\").read())\n",
    "\n",
    "        # Get all other articles - check both old and new path fields\n",
    "        similar_articles = []\n",
    "        all_articles = self.collection.find(\n",
    "            {\n",
    "                \"$or\": [\n",
    "                    {\"relative_path\": {\"$ne\": relative_path}},\n",
    "                    {\"file_path\": {\"$exists\": True}},  # For backwards compatibility\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for article in all_articles:\n",
    "            # Handle both old and new path formats\n",
    "            try:\n",
    "                if \"relative_path\" in article:\n",
    "                    other_path = article[\"relative_path\"]\n",
    "                else:\n",
    "                    # Skip if it's the same article\n",
    "                    if article[\"file_path\"] == full_path:\n",
    "                        continue\n",
    "                    # Convert old file_path to relative_path\n",
    "                    other_path = str(\n",
    "                        Path(article[\"file_path\"]).relative_to(self.content_base_path)\n",
    "                    )\n",
    "\n",
    "                other_full_path = self.get_full_path(other_path)\n",
    "                other_content = exclude_front_matter(open(other_full_path, \"r\").read())\n",
    "\n",
    "                # Calculate similarity ratio\n",
    "                similarity = SequenceMatcher(\n",
    "                    None, current_content, other_content\n",
    "                ).ratio()\n",
    "\n",
    "                if similarity >= similarity_threshold:\n",
    "                    similar_articles.append(\n",
    "                        {\n",
    "                            \"relative_path\": other_path,\n",
    "                            \"similarity\": similarity,\n",
    "                            \"title\": article[\"metadata\"][\"title\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(f\"Warning: Skipping article due to path error: {e}\")\n",
    "                continue\n",
    "\n",
    "        result = {\n",
    "            \"has_duplicates\": len(similar_articles) > 0,\n",
    "            \"similar_articles\": similar_articles,\n",
    "        }\n",
    "\n",
    "        self.collection.update_one(\n",
    "            {\"relative_path\": relative_path},\n",
    "            {\"$set\": {\"duplicate_content_analysis\": result}},\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def analyze_keyword_cannibalization(self, focus_keyword: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect keyword cannibalization across articles\"\"\"\n",
    "        # Find all articles using this focus keyword\n",
    "        articles = list(\n",
    "            self.collection.find(\n",
    "                {\n",
    "                    \"$or\": [\n",
    "                        {\"focus_keyword\": focus_keyword},\n",
    "                        {\"related_keywords\": focus_keyword},\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if len(articles) <= 1:\n",
    "            return {\n",
    "                \"has_cannibalization\": False,\n",
    "                \"keyword\": focus_keyword,\n",
    "                \"count\": len(articles),\n",
    "            }\n",
    "\n",
    "        cannibalization_data = {\n",
    "            \"has_cannibalization\": True,\n",
    "            \"keyword\": focus_keyword,\n",
    "            \"count\": len(articles),\n",
    "            \"articles\": [],\n",
    "        }\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Handle both old and new path formats\n",
    "                if \"relative_path\" in article:\n",
    "                    article_path = article[\"relative_path\"]\n",
    "                else:\n",
    "                    # Convert old file_path to relative_path\n",
    "                    article_path = str(\n",
    "                        Path(article[\"file_path\"]).relative_to(self.content_base_path)\n",
    "                    )\n",
    "\n",
    "                density = self.calculate_keyword_density(article_path)\n",
    "\n",
    "                article_data = {\n",
    "                    \"relative_path\": article_path,\n",
    "                    \"title\": article[\"metadata\"][\"title\"],\n",
    "                    \"publish_date\": article[\"metadata\"][\"publishDate\"],\n",
    "                    \"keyword_type\": (\n",
    "                        \"focus\"\n",
    "                        if article.get(\"focus_keyword\") == focus_keyword\n",
    "                        else \"related\"\n",
    "                    ),\n",
    "                    \"density\": (\n",
    "                        density[\"focus_keyword\"][\"density\"]\n",
    "                        if article.get(\"focus_keyword\") == focus_keyword\n",
    "                        else next(\n",
    "                            (\n",
    "                                kw[\"density\"]\n",
    "                                for kw in density[\"related_keywords\"]\n",
    "                                if kw[\"keyword\"] == focus_keyword\n",
    "                            ),\n",
    "                            0,\n",
    "                        )\n",
    "                    ),\n",
    "                }\n",
    "                cannibalization_data[\"articles\"].append(article_data)\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(\n",
    "                    f\"Warning: Skipping article in cannibalization analysis due to error: {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        return cannibalization_data\n",
    "    def analyze_content_groups(self, similarity_threshold: float = 0.8) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze and group similar articles\"\"\"\n",
    "        all_articles = list(self.collection.find())\n",
    "        content_groups = []\n",
    "        processed = set()\n",
    "        \n",
    "        for article in all_articles:\n",
    "            article_path = article.get('relative_path')\n",
    "            if article_path in processed:\n",
    "                continue\n",
    "                \n",
    "            # Start a new group\n",
    "            group = {\n",
    "                'main_article': {\n",
    "                    'title': article['metadata']['title'],\n",
    "                    'path': article_path\n",
    "                },\n",
    "                'similar_articles': [],\n",
    "                'common_content': [],\n",
    "                'unique_content': []\n",
    "            }\n",
    "            \n",
    "            # Get content of main article\n",
    "            main_content = exclude_front_matter(open(self.get_full_path(article_path), 'r').read())\n",
    "            main_content_lines = main_content.split('\\n')\n",
    "            \n",
    "            for other in all_articles:\n",
    "                if other.get('relative_path') == article_path:\n",
    "                    continue\n",
    "                    \n",
    "                other_path = other.get('relative_path')\n",
    "                other_content = exclude_front_matter(open(self.get_full_path(other_path), 'r').read())\n",
    "                \n",
    "                # Calculate similarity\n",
    "                similarity = SequenceMatcher(None, main_content, other_content).ratio()\n",
    "                \n",
    "                if similarity >= similarity_threshold:\n",
    "                    processed.add(other_path)\n",
    "                    group['similar_articles'].append({\n",
    "                        'title': other['metadata']['title'],\n",
    "                        'path': other_path,\n",
    "                        'similarity': similarity\n",
    "                    })\n",
    "                    \n",
    "                    # Find common content\n",
    "                    other_lines = other_content.split('\\n')\n",
    "                    common_lines = set()\n",
    "                    for line in main_content_lines:\n",
    "                        if line.strip() and line in other_lines:\n",
    "                            common_lines.add(line)\n",
    "                    \n",
    "                    group['common_content'] = list(common_lines)\n",
    "                    \n",
    "                    # Find unique content in main article\n",
    "                    unique_lines = set()\n",
    "                    for line in main_content_lines:\n",
    "                        if line.strip() and line not in other_lines:\n",
    "                            unique_lines.add(line)\n",
    "                    \n",
    "                    group['unique_content'] = list(unique_lines)\n",
    "            \n",
    "            if group['similar_articles']:\n",
    "                content_groups.append(group)\n",
    "                processed.add(article_path)\n",
    "        \n",
    "        return {\n",
    "            'total_articles': len(all_articles),\n",
    "            'groups': content_groups,\n",
    "            'unique_articles': len(all_articles) - len(processed),\n",
    "            'duplicate_groups': len(content_groups)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
