{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime, date\n",
    "from typing import Optional, Dict, Any, List\n",
    "from pathlib import Path\n",
    "from pymongo.database import Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seo_rat.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Article:\n",
    "    \"\"\"Class representing an article's metadata and analysis\"\"\"\n",
    "\n",
    "    # File information (only required fields)\n",
    "    relative_path: str  # Path relative to content directory\n",
    "    website_domain: str\n",
    "    base_path: str\n",
    "\n",
    "    # Frontmatter data (will be populated from file)\n",
    "    title: Optional[str] = None\n",
    "    publishDate: Optional[str] = None\n",
    "    excerpt: Optional[str] = None\n",
    "    image: Optional[str] = None\n",
    "    tags: Optional[List[str]] = None\n",
    "    # Focus keyword\n",
    "    focus_keyword: Optional[str] = None\n",
    "    related_keywords: List[str] = None\n",
    "\n",
    "    # Analysis data (will be populated from content)\n",
    "    headers: List[Dict[str, Any]] = None\n",
    "    internal_links: Dict[str, Dict] = None\n",
    "    external_links: Dict[str, Dict] = None\n",
    "    images_data: List[Dict[str, str]] = None\n",
    "    phone_numbers: List[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Load and analyze file content\"\"\"\n",
    "        self.load_from_file()\n",
    "\n",
    "    def get_full_path(self) -> str:\n",
    "        \"\"\"Get full file path\"\"\"\n",
    "        return str(Path(self.base_path) / self.relative_path)\n",
    "\n",
    "    def load_from_file(self):\n",
    "        \"\"\"Load and parse file content\"\"\"\n",
    "        with open(self.get_full_path(), \"r\") as f:\n",
    "            content = f.read()\n",
    "        # Extract frontmatter\n",
    "        self.title, self.publishDate, self.excerpt, self.image, self.tags = (\n",
    "            extract_frontmatter(content)\n",
    "        )\n",
    "\n",
    "        # Get content without frontmatter\n",
    "        main_content = exclude_front_matter(content)\n",
    "\n",
    "        # Analyze content\n",
    "        self.analyze_content(main_content)\n",
    "\n",
    "    def analyze_content(self, content: str):\n",
    "        \"\"\"Analyze the content and populate metadata\"\"\"\n",
    "        self.headers = get_heads_info(self.get_full_path())  # Changed from file_path\n",
    "\n",
    "        urls_data = extract_markdown_urls_with_tags(content)\n",
    "        self.internal_links = {\n",
    "            url: data\n",
    "            for url, data in urls_data.items()\n",
    "            if url in get_internal_urls([url], self.website_domain)\n",
    "        }\n",
    "        self.external_links = {\n",
    "            url: data\n",
    "            for url, data in urls_data.items()\n",
    "            if url in get_external_urls([url], self.website_domain)\n",
    "        }\n",
    "        self.images_data = extract_markdown_images(content)\n",
    "        self.phone_numbers = detect_numbers(content)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert article data to MongoDB document format\"\"\"\n",
    "        # Convert publishDate to ISO format string if it's a datetime\n",
    "        publish_date = self.publishDate\n",
    "        if isinstance(publish_date, (datetime, date)):\n",
    "            publish_date = publish_date.isoformat()\n",
    "\n",
    "        return {\n",
    "            \"relative_path\": self.relative_path,  # Changed from file_path\n",
    "            \"base_path\": self.base_path,\n",
    "            \"website_domain\": self.website_domain,\n",
    "            \"focus_keyword\": self.focus_keyword,\n",
    "            \"metadata\": {\n",
    "                \"title\": self.title,\n",
    "                \"publishDate\": publish_date,  # Use converted date\n",
    "                \"excerpt\": self.excerpt,\n",
    "                \"image\": self.image,\n",
    "                \"tags\": self.tags,\n",
    "            },\n",
    "            \"analysis\": {\n",
    "                \"headers\": self.headers,\n",
    "                \"internal_links\": self.internal_links,\n",
    "                \"external_links\": self.external_links,\n",
    "                \"images\": self.images_data,\n",
    "                \"phone_numbers\": self.phone_numbers,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test environment...\n",
      "Creating Article instance...\n",
      "\n",
      "Testing basic attributes...\n",
      "\n",
      "Testing content analysis...\n",
      "\n",
      "Testing links...\n",
      "\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "#|test\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "test_content = \"\"\"---\n",
    "publishDate: 2024-01-27\n",
    "title: Test Article\n",
    "excerpt: Test excerpt\n",
    "image: ~/assets/images/test.png\n",
    "tags:\n",
    "  - Test\n",
    "  - Demo\n",
    "---\n",
    "# Main Heading\n",
    "This is a test article with a [link](https://example.com) and an ![image](test.png).\n",
    "## Second Heading\n",
    "Contact us at +966503139675\n",
    "[internal link](https://testdomain.com/page)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Setting up test environment...\")\n",
    "# Create temporary test file\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.md') as temp_file:\n",
    "    temp_file.write(test_content)\n",
    "    test_file_path = temp_file.name\n",
    "\n",
    "# Create Article instance\n",
    "print(\"Creating Article instance...\")\n",
    "article = Article(\n",
    "    relative_path=Path(test_file_path).name,\n",
    "    website_domain=\"testdomain.com\",\n",
    "    base_path=str(Path(test_file_path).parent)\n",
    ")\n",
    "\n",
    "# Test basic attributes\n",
    "print(\"\\nTesting basic attributes...\")\n",
    "assert article.title == \"Test Article\"\n",
    "assert str(article.publishDate) == \"2024-01-27\"\n",
    "assert article.tags == [\"Test\", \"Demo\"]\n",
    "\n",
    "# Test content analysis\n",
    "print(\"\\nTesting content analysis...\")\n",
    "assert len(article.headers) > 0\n",
    "assert any(h['content'] == 'Main Heading' for h in article.headers)\n",
    "assert len(article.phone_numbers) > 0\n",
    "assert '966503139' in article.phone_numbers\n",
    "\n",
    "# Test links\n",
    "print(\"\\nTesting links...\")\n",
    "assert len(article.external_links) > 0\n",
    "assert len(article.internal_links) > 0\n",
    "assert 'https://example.com' in article.external_links\n",
    "\n",
    "# Cleanup\n",
    "Path(test_file_path).unlink()\n",
    "print(\"\\nAll tests passed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "class ArticleStore:\n",
    "    \"\"\"Handle MongoDB operations for Article objects\"\"\"\n",
    "\n",
    "    def __init__(self, db: Database, content_base_path: str):\n",
    "        self.db = db\n",
    "        self.collection = self.db.articles\n",
    "        self.content_base_path = Path(content_base_path)\n",
    "\n",
    "    def check_duplicate_before_insert(\n",
    "        self, article: Article, similarity_threshold: float = 0.8\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Check if article content is duplicate before insertion\"\"\"\n",
    "        current_content = exclude_front_matter(\n",
    "            open(article.get_full_path(), \"r\").read()\n",
    "        )\n",
    "        current_content = re.sub(r\"\\s+\", \" \", current_content).strip()\n",
    "\n",
    "        # Check existing articles\n",
    "        existing_articles = self.collection.find()\n",
    "        duplicates = []\n",
    "\n",
    "        for existing in existing_articles:\n",
    "            try:\n",
    "                # Skip if it's the same article (same website and path)\n",
    "                if (\n",
    "                    existing.get(\"website_domain\") == article.website_domain\n",
    "                    and existing.get(\"relative_path\") == article.relative_path\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                # Get existing article content\n",
    "                if \"relative_path\" in existing:\n",
    "                    existing_path = self.get_full_path(existing[\"relative_path\"])\n",
    "                else:\n",
    "                    existing_path = existing[\"file_path\"]\n",
    "\n",
    "                existing_content = exclude_front_matter(open(existing_path, \"r\").read())\n",
    "                existing_content = re.sub(r\"\\s+\", \" \", existing_content).strip()\n",
    "\n",
    "                # Calculate similarity\n",
    "                similarity = SequenceMatcher(\n",
    "                    None, current_content, existing_content\n",
    "                ).ratio()\n",
    "\n",
    "                if similarity >= similarity_threshold:\n",
    "                    duplicates.append(\n",
    "                        {\n",
    "                            \"title\": existing[\"metadata\"][\"title\"],\n",
    "                            \"similarity\": similarity,\n",
    "                            \"path\": existing.get(\"relative_path\")\n",
    "                            or existing.get(\"file_path\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: Error checking article {existing.get('relative_path', 'unknown')}: {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        return {\"has_duplicates\": bool(duplicates), \"duplicates\": duplicates}\n",
    "\n",
    "    def insert_or_update_article(self, article: Article) -> str:\n",
    "        \"\"\"Insert article if not exists, or update if exists based on website and path\"\"\"\n",
    "        # First check for duplicates\n",
    "        duplicate_check = self.check_duplicate_before_insert(article)\n",
    "        if duplicate_check[\"has_duplicates\"]:\n",
    "            print(\"Warning: Similar content detected:\")\n",
    "            for dup in duplicate_check[\"duplicates\"]:\n",
    "                print(f\"- {dup['title']} (similarity: {dup['similarity']:.2f})\")\n",
    "\n",
    "        article_dict = article.to_dict()\n",
    "\n",
    "        # Try to update existing article\n",
    "        result = self.collection.update_one(\n",
    "            {\n",
    "                \"website_domain\": article.website_domain,\n",
    "                \"relative_path\": article.relative_path,\n",
    "            },\n",
    "            {\"$set\": article_dict},\n",
    "            upsert=True,\n",
    "        )\n",
    "\n",
    "        if result.upserted_id:\n",
    "            return str(result.upserted_id)\n",
    "        else:\n",
    "            # Get the _id of the existing document\n",
    "            existing = self.collection.find_one(\n",
    "                {\n",
    "                    \"website_domain\": article.website_domain,\n",
    "                    \"relative_path\": article.relative_path,\n",
    "                }\n",
    "            )\n",
    "            return str(existing[\"_id\"])\n",
    "\n",
    "    def get_all_article_files(self) -> List[str]:\n",
    "        \"\"\"Get all markdown files in content directory\"\"\"\n",
    "        pattern = str(self.content_base_path / \"**/*.md\")\n",
    "        files = glob.glob(pattern, recursive=True)\n",
    "        return [str(Path(f).relative_to(self.content_base_path)) for f in files]\n",
    "\n",
    "    def find_by_tag(self, tag: str) -> List[Dict]:\n",
    "        \"\"\"Find articles by tag\"\"\"\n",
    "        return list(self.collection.find({\"metadata.tags\": tag}))\n",
    "\n",
    "    def find_articles_with_phone_numbers(self) -> List[Dict]:\n",
    "        \"\"\"Find articles containing phone numbers\"\"\"\n",
    "        return list(\n",
    "            self.collection.find(\n",
    "                {\"analysis.phone_numbers\": {\"$exists\": True, \"$ne\": []}}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_full_path(self, relative_path: str) -> str:\n",
    "        \"\"\"Convert relative path to full path\"\"\"\n",
    "        return str(self.content_base_path / relative_path)\n",
    "\n",
    "    def find_by_domain(self, domain: str) -> List[Dict]:\n",
    "        \"\"\"Find all articles for a specific website domain\"\"\"\n",
    "        return list(self.collection.find({\"website_domain\": domain}))\n",
    "\n",
    "    def search_in_title(self, keyword: str) -> List[Dict]:\n",
    "        \"\"\"Search for keyword in article titles\"\"\"\n",
    "        return list(\n",
    "            self.collection.find(\n",
    "                {\"metadata.title\": {\"$regex\": keyword, \"$options\": \"i\"}}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def set_article_keywords(\n",
    "        self, relative_path: str, focus_keyword: str, related_keywords: List[str] = None\n",
    "    ) -> bool:\n",
    "        \"\"\"Set focus keyword and related keywords for an article\"\"\"\n",
    "        if related_keywords is None:\n",
    "            related_keywords = []\n",
    "        result = self.collection.update_one(\n",
    "            {\"relative_path\": relative_path},\n",
    "            {\n",
    "                \"$set\": {\n",
    "                    \"focus_keyword\": focus_keyword,\n",
    "                    \"related_keywords\": related_keywords,\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        return result.modified_count > 0\n",
    "\n",
    "    def get_article_metadata(self, relative_path: str) -> Dict:\n",
    "        \"\"\"Get complete metadata for a specific article\"\"\"\n",
    "        article = self.collection.find_one({\"relative_path\": relative_path})\n",
    "        if article is None:\n",
    "            raise ValueError(f\"No article found with relative_path: {relative_path}\")\n",
    "        return article\n",
    "\n",
    "    def get_article_analysis(self, relative_path: str) -> Dict:\n",
    "        \"\"\"Get just the analysis part of an article\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        return article.get(\"analysis\", {})\n",
    "\n",
    "    def get_article_headers(self, relative_path: str) -> List[Dict]:\n",
    "        \"\"\"Get headers from specific article\"\"\"\n",
    "        analysis = self.get_article_analysis(relative_path)\n",
    "        return analysis.get(\"headers\", [])\n",
    "\n",
    "    def get_article_links(self, relative_path: str) -> Dict:\n",
    "        \"\"\"Get all links from specific article\"\"\"\n",
    "        analysis = self.get_article_analysis(relative_path)\n",
    "        return {\n",
    "            \"internal\": analysis.get(\"internal_links\", {}),\n",
    "            \"external\": analysis.get(\"external_links\", {}),\n",
    "        }\n",
    "\n",
    "    def get_article_phone_numbers(self, relative_path: str) -> List[str]:\n",
    "        \"\"\"Get phone numbers from specific article\"\"\"\n",
    "        analysis = self.get_article_analysis(relative_path)\n",
    "        return analysis.get(\"phone_numbers\", [])\n",
    "\n",
    "    def get_article_images(self, relative_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get images data from specific article\"\"\"\n",
    "        analysis = self.get_article_analysis(relative_path)\n",
    "        return analysis.get(\"images\", [])\n",
    "\n",
    "    def analyze_header_hierarchy(self, relative_path: str) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Analyze headers by level\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        headers = article[\"analysis\"][\"headers\"]\n",
    "        return {\n",
    "            \"h1\": [h for h in headers if h[\"type\"] == \"h1\"],\n",
    "            \"h2\": [h for h in headers if h[\"type\"] == \"h2\"],\n",
    "            \"h3\": [h for h in headers if h[\"type\"] == \"h3\"],\n",
    "            \"h4\": [h for h in headers if h[\"type\"] == \"h4\"],\n",
    "            \"h5\": [h for h in headers if h[\"type\"] == \"h5\"],\n",
    "            \"h6\": [h for h in headers if h[\"type\"] == \"h6\"],\n",
    "        }\n",
    "\n",
    "    def calculate_keyword_density(self, relative_path: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate keyword density for focus and related keywords\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        keywords = self.get_article_keywords(relative_path)\n",
    "        full_path = self.get_full_path(relative_path)\n",
    "        content = exclude_front_matter(open(full_path, \"r\").read())\n",
    "\n",
    "        # Convert content to lowercase for case-insensitive matching\n",
    "        content_lower = content.lower()\n",
    "        # Get total word count\n",
    "        total_words = len(content.split())\n",
    "\n",
    "        densities = {}\n",
    "\n",
    "        # Calculate focus keyword density\n",
    "        if keywords.get(\"focus_keyword\"):\n",
    "            focus_kw = keywords[\"focus_keyword\"].lower()\n",
    "            focus_count = content_lower.count(focus_kw)\n",
    "            densities[\"focus_keyword\"] = {\n",
    "                \"keyword\": keywords[\"focus_keyword\"],\n",
    "                \"count\": focus_count,\n",
    "                \"density\": (focus_count / total_words) * 100 if total_words > 0 else 0,\n",
    "            }\n",
    "\n",
    "        # Calculate related keywords density\n",
    "        densities[\"related_keywords\"] = []\n",
    "        for kw in keywords.get(\"related_keywords\", []):\n",
    "            kw_lower = kw.lower()\n",
    "            count = content_lower.count(kw_lower)\n",
    "            densities[\"related_keywords\"].append(\n",
    "                {\n",
    "                    \"keyword\": kw,\n",
    "                    \"count\": count,\n",
    "                    \"density\": (count / total_words) * 100 if total_words > 0 else 0,\n",
    "                }\n",
    "            )\n",
    "        self.collection.update_one(\n",
    "            {\"relative_path\": relative_path}, {\"$set\": {\"keyword_density\": densities}}\n",
    "        )\n",
    "        return densities\n",
    "\n",
    "    def get_article_keywords(self, relative_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get keywords data for an article\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        return {\n",
    "            \"focus_keyword\": article.get(\"focus_keyword\"),\n",
    "            \"related_keywords\": article.get(\"related_keywords\", []),\n",
    "        }\n",
    "\n",
    "    def analyze_seo_rules(self, relative_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze SEO rules for a specific article\"\"\"\n",
    "        article = self.get_article_metadata(relative_path)\n",
    "        keywords = self.get_article_keywords(relative_path)\n",
    "        full_path = self.get_full_path(relative_path)\n",
    "        focus_keyword = keywords.get(\"focus_keyword\")\n",
    "        related_keywords = keywords.get(\"related_keywords\", [])\n",
    "\n",
    "        # Get all headers of type h1\n",
    "        h1_headers = [h for h in article[\"analysis\"][\"headers\"] if h[\"type\"] == \"h1\"]\n",
    "\n",
    "        # Check if focus keyword is in excerpt\n",
    "        excerpt = article[\"metadata\"].get(\"excerpt\", \"\")\n",
    "\n",
    "        # Get content for 10% analysis\n",
    "        content = exclude_front_matter(open(full_path, \"r\").read())\n",
    "        first_10_percent = content[: int(len(content) * 0.1)]\n",
    "\n",
    "        # Get image alt texts\n",
    "        alt_texts = [img[\"alt_text\"] for img in article[\"analysis\"][\"images\"]]\n",
    "\n",
    "        # Check if focus keyword is unique across all articles\n",
    "        other_articles = list(\n",
    "            self.collection.find(\n",
    "                {\n",
    "                    \"relative_path\": {\"$ne\": relative_path},\n",
    "                    \"focus_keyword\": focus_keyword,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Count and find positions of focus keyword\n",
    "        keyword_positions = []\n",
    "        content_lower = content.lower()\n",
    "        focus_keyword_lower = focus_keyword.lower() if focus_keyword else \"\"\n",
    "        pos = 0\n",
    "        while (\n",
    "            focus_keyword\n",
    "            and (pos := content_lower.find(focus_keyword_lower, pos)) != -1\n",
    "        ):\n",
    "            keyword_positions.append(pos)\n",
    "            pos += 1\n",
    "\n",
    "        analysis = {\n",
    "            \"h1_count\": len(h1_headers),\n",
    "            \"has_single_h1\": len(h1_headers) == 1,\n",
    "            \"focus_keyword_in_excerpt\": (\n",
    "                focus_keyword and focus_keyword.lower() in excerpt.lower()\n",
    "                if excerpt\n",
    "                else False\n",
    "            ),\n",
    "            \"keywords_in_first_10_percent\": {\n",
    "                \"focus_keyword\": focus_keyword\n",
    "                and focus_keyword.lower() in first_10_percent.lower(),\n",
    "                \"related_keywords\": [\n",
    "                    kw\n",
    "                    for kw in related_keywords\n",
    "                    if kw.lower() in first_10_percent.lower()\n",
    "                ],\n",
    "            },\n",
    "            \"keywords_in_alt_text\": {\n",
    "                \"focus_keyword\": any(\n",
    "                    focus_keyword and focus_keyword.lower() in alt.lower()\n",
    "                    for alt in alt_texts\n",
    "                ),\n",
    "                \"related_keywords\": [\n",
    "                    kw\n",
    "                    for kw in related_keywords\n",
    "                    if any(kw.lower() in alt.lower() for alt in alt_texts)\n",
    "                ],\n",
    "            },\n",
    "            \"has_external_links\": bool(article[\"analysis\"][\"external_links\"]),\n",
    "            \"focus_keyword_is_unique\": len(other_articles) == 0,  # Changed this line\n",
    "            \"focus_keyword_stats\": {\n",
    "                \"count\": len(keyword_positions),\n",
    "                \"positions\": keyword_positions,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Update article with analysis\n",
    "        self.collection.update_one(\n",
    "            {\"relative_path\": relative_path}, {\"$set\": {\"seo_analysis\": analysis}}\n",
    "        )\n",
    "        return analysis\n",
    "\n",
    "    def detect_duplicate_content(\n",
    "        self, relative_path: str, similarity_threshold: float = 0.8\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Detect similar content across articles\"\"\"\n",
    "        current_article = self.get_article_metadata(relative_path)\n",
    "        full_path = self.get_full_path(relative_path)\n",
    "        current_content = exclude_front_matter(open(full_path, \"r\").read())\n",
    "\n",
    "        # Get all other articles - check both old and new path fields\n",
    "        similar_articles = []\n",
    "        all_articles = self.collection.find(\n",
    "            {\n",
    "                \"$or\": [\n",
    "                    {\"relative_path\": {\"$ne\": relative_path}},\n",
    "                    {\"file_path\": {\"$exists\": True}},  # For backwards compatibility\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for article in all_articles:\n",
    "            # Handle both old and new path formats\n",
    "            try:\n",
    "                if \"relative_path\" in article:\n",
    "                    other_path = article[\"relative_path\"]\n",
    "                else:\n",
    "                    # Skip if it's the same article\n",
    "                    if article[\"file_path\"] == full_path:\n",
    "                        continue\n",
    "                    # Convert old file_path to relative_path\n",
    "                    other_path = str(\n",
    "                        Path(article[\"file_path\"]).relative_to(self.content_base_path)\n",
    "                    )\n",
    "\n",
    "                other_full_path = self.get_full_path(other_path)\n",
    "                other_content = exclude_front_matter(open(other_full_path, \"r\").read())\n",
    "\n",
    "                # Calculate similarity ratio\n",
    "                similarity = SequenceMatcher(\n",
    "                    None, current_content, other_content\n",
    "                ).ratio()\n",
    "\n",
    "                if similarity >= similarity_threshold:\n",
    "                    similar_articles.append(\n",
    "                        {\n",
    "                            \"relative_path\": other_path,\n",
    "                            \"similarity\": similarity,\n",
    "                            \"title\": article[\"metadata\"][\"title\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(f\"Warning: Skipping article due to path error: {e}\")\n",
    "                continue\n",
    "\n",
    "        result = {\n",
    "            \"has_duplicates\": len(similar_articles) > 0,\n",
    "            \"similar_articles\": similar_articles,\n",
    "        }\n",
    "\n",
    "        self.collection.update_one(\n",
    "            {\"relative_path\": relative_path},\n",
    "            {\"$set\": {\"duplicate_content_analysis\": result}},\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def analyze_keyword_cannibalization(self, focus_keyword: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect keyword cannibalization across articles\"\"\"\n",
    "        # Find all articles using this focus keyword\n",
    "        articles = list(\n",
    "            self.collection.find(\n",
    "                {\n",
    "                    \"$or\": [\n",
    "                        {\"focus_keyword\": focus_keyword},\n",
    "                        {\"related_keywords\": focus_keyword},\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if len(articles) <= 1:\n",
    "            return {\n",
    "                \"has_cannibalization\": False,\n",
    "                \"keyword\": focus_keyword,\n",
    "                \"count\": len(articles),\n",
    "            }\n",
    "\n",
    "        cannibalization_data = {\n",
    "            \"has_cannibalization\": True,\n",
    "            \"keyword\": focus_keyword,\n",
    "            \"count\": len(articles),\n",
    "            \"articles\": [],\n",
    "        }\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Handle both old and new path formats\n",
    "                if \"relative_path\" in article:\n",
    "                    article_path = article[\"relative_path\"]\n",
    "                else:\n",
    "                    # Convert old file_path to relative_path\n",
    "                    article_path = str(\n",
    "                        Path(article[\"file_path\"]).relative_to(self.content_base_path)\n",
    "                    )\n",
    "\n",
    "                density = self.calculate_keyword_density(article_path)\n",
    "\n",
    "                article_data = {\n",
    "                    \"relative_path\": article_path,\n",
    "                    \"title\": article[\"metadata\"][\"title\"],\n",
    "                    \"publish_date\": article[\"metadata\"][\"publishDate\"],\n",
    "                    \"keyword_type\": (\n",
    "                        \"focus\"\n",
    "                        if article.get(\"focus_keyword\") == focus_keyword\n",
    "                        else \"related\"\n",
    "                    ),\n",
    "                    \"density\": (\n",
    "                        density[\"focus_keyword\"][\"density\"]\n",
    "                        if article.get(\"focus_keyword\") == focus_keyword\n",
    "                        else next(\n",
    "                            (\n",
    "                                kw[\"density\"]\n",
    "                                for kw in density[\"related_keywords\"]\n",
    "                                if kw[\"keyword\"] == focus_keyword\n",
    "                            ),\n",
    "                            0,\n",
    "                        )\n",
    "                    ),\n",
    "                }\n",
    "                cannibalization_data[\"articles\"].append(article_data)\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                print(\n",
    "                    f\"Warning: Skipping article in cannibalization analysis due to error: {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        return cannibalization_data\n",
    "    def analyze_content_groups(self, similarity_threshold: float = 0.8) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze and group similar articles\"\"\"\n",
    "        all_articles = list(self.collection.find())\n",
    "        content_groups = []\n",
    "        processed = set()\n",
    "        \n",
    "        for article in all_articles:\n",
    "            article_path = article.get('relative_path')\n",
    "            if article_path in processed:\n",
    "                continue\n",
    "                \n",
    "            # Start a new group\n",
    "            group = {\n",
    "                'main_article': {\n",
    "                    'title': article['metadata']['title'],\n",
    "                    'path': article_path\n",
    "                },\n",
    "                'similar_articles': [],\n",
    "                'common_content': [],\n",
    "                'unique_content': []\n",
    "            }\n",
    "            \n",
    "            # Get content of main article\n",
    "            main_content = exclude_front_matter(open(self.get_full_path(article_path), 'r').read())\n",
    "            main_content_lines = main_content.split('\\n')\n",
    "            \n",
    "            for other in all_articles:\n",
    "                if other.get('relative_path') == article_path:\n",
    "                    continue\n",
    "                    \n",
    "                other_path = other.get('relative_path')\n",
    "                other_content = exclude_front_matter(open(self.get_full_path(other_path), 'r').read())\n",
    "                \n",
    "                # Calculate similarity\n",
    "                similarity = SequenceMatcher(None, main_content, other_content).ratio()\n",
    "                \n",
    "                if similarity >= similarity_threshold:\n",
    "                    processed.add(other_path)\n",
    "                    group['similar_articles'].append({\n",
    "                        'title': other['metadata']['title'],\n",
    "                        'path': other_path,\n",
    "                        'similarity': similarity\n",
    "                    })\n",
    "                    \n",
    "                    # Find common content\n",
    "                    other_lines = other_content.split('\\n')\n",
    "                    common_lines = set()\n",
    "                    for line in main_content_lines:\n",
    "                        if line.strip() and line in other_lines:\n",
    "                            common_lines.add(line)\n",
    "                    \n",
    "                    group['common_content'] = list(common_lines)\n",
    "                    \n",
    "                    # Find unique content in main article\n",
    "                    unique_lines = set()\n",
    "                    for line in main_content_lines:\n",
    "                        if line.strip() and line not in other_lines:\n",
    "                            unique_lines.add(line)\n",
    "                    \n",
    "                    group['unique_content'] = list(unique_lines)\n",
    "            \n",
    "            if group['similar_articles']:\n",
    "                content_groups.append(group)\n",
    "                processed.add(article_path)\n",
    "        \n",
    "        return {\n",
    "            'total_articles': len(all_articles),\n",
    "            'groups': content_groups,\n",
    "            'unique_articles': len(all_articles) - len(processed),\n",
    "            'duplicate_groups': len(content_groups)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test environment...\n",
      "Creating ArticleStore instance...\n",
      "\n",
      "Testing insert_or_update_article...\n",
      "\n",
      "All basic tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "#|test\n",
    "from unittest.mock import Mock, MagicMock\n",
    "from bson import ObjectId\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "print(\"Setting up test environment...\")\n",
    "\n",
    "# Create test content and temporary directory\n",
    "test_content = \"\"\"---\n",
    "publishDate: 2024-01-27\n",
    "title: Test Article\n",
    "excerpt: Test excerpt\n",
    "tags:\n",
    "  - Test\n",
    "---\n",
    "# Test Content\n",
    "Some content here\n",
    "\"\"\"\n",
    "\n",
    "# Create mock database\n",
    "mock_collection = MagicMock()\n",
    "mock_db = MagicMock()\n",
    "mock_db.articles = mock_collection\n",
    "\n",
    "# Create temporary directory for content\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Create a test file\n",
    "    test_file_path = os.path.join(temp_dir, \"test.md\")\n",
    "    with open(test_file_path, \"w\") as f:\n",
    "        f.write(test_content)\n",
    "    \n",
    "    print(\"Creating ArticleStore instance...\")\n",
    "    store = ArticleStore(mock_db, temp_dir)\n",
    "    \n",
    "    # Test insert_or_update_article\n",
    "    print(\"\\nTesting insert_or_update_article...\")\n",
    "    article = Article(\n",
    "        relative_path=\"test.md\",\n",
    "        website_domain=\"test.com\",\n",
    "        base_path=temp_dir\n",
    "    )\n",
    "    \n",
    "    mock_collection.update_one.return_value = MagicMock(\n",
    "        upserted_id=ObjectId(\"507f1f77bcf86cd799439011\")\n",
    "    )\n",
    "    \n",
    "    result_id = store.insert_or_update_article(article)\n",
    "    assert result_id == \"507f1f77bcf86cd799439011\"\n",
    "\n",
    "print(\"\\nAll basic tests passed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test environment...\n",
      "\n",
      "Testing find_by_tag...\n",
      "\n",
      "Testing search_in_title...\n",
      "\n",
      "Testing find_by_domain...\n",
      "\n",
      "All search tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "#|test\n",
    "from unittest.mock import Mock, MagicMock\n",
    "from bson import ObjectId\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "print(\"Setting up test environment...\")\n",
    "\n",
    "# Create test content\n",
    "test_content = \"\"\"---\n",
    "publishDate: 2024-01-27\n",
    "title: Test Article\n",
    "excerpt: Test excerpt\n",
    "tags:\n",
    "  - Python\n",
    "  - Testing\n",
    "---\n",
    "# Main Heading\n",
    "Contact us: +966503139675\n",
    "[Link](https://example.com)\n",
    "\"\"\"\n",
    "\n",
    "# Mock database setup\n",
    "mock_collection = MagicMock()\n",
    "mock_db = MagicMock()\n",
    "mock_db.articles = mock_collection\n",
    "\n",
    "# Test directory setup\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    test_file_path = os.path.join(temp_dir, \"test.md\")\n",
    "    with open(test_file_path, \"w\") as f:\n",
    "        f.write(test_content)\n",
    "    \n",
    "    store = ArticleStore(mock_db, temp_dir)\n",
    "    \n",
    "    print(\"\\nTesting find_by_tag...\")\n",
    "    mock_collection.find.return_value = [{\"title\": \"Test Article\"}]\n",
    "    results = store.find_by_tag(\"Python\")\n",
    "    assert len(results) > 0\n",
    "    \n",
    "    print(\"\\nTesting search_in_title...\")\n",
    "    mock_collection.find.return_value = [{\"metadata\": {\"title\": \"Test Article\"}}]\n",
    "    results = store.search_in_title(\"Test\")\n",
    "    assert len(results) > 0\n",
    "    \n",
    "    print(\"\\nTesting find_by_domain...\")\n",
    "    mock_collection.find.return_value = [{\"website_domain\": \"test.com\"}]\n",
    "    results = store.find_by_domain(\"test.com\")\n",
    "    assert len(results) > 0\n",
    "\n",
    "print(\"\\nAll search tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test environment...\n",
      "\n",
      "Testing analyze_seo_rules...\n",
      "# Main Python \n",
      "Checking SEO analysis results...\n",
      "Checking keyword placement...\n",
      "Checking image optimization...\n",
      "Checking keyword uniqueness...\n",
      "Checking keyword statistics...\n",
      "Checking external links...\n",
      "\n",
      "All comprehensive SEO analysis tests passed successfully!\n",
      "\n",
      "All SEO analysis tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "#|test\n",
    "from unittest.mock import Mock, MagicMock\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "print(\"Setting up test environment...\")\n",
    "\n",
    "# Create test content with specific SEO elements\n",
    "test_content = \"\"\"---\n",
    "publishDate: 2024-01-27\n",
    "title: Test SEO Article\n",
    "excerpt: This is a test article about python programming\n",
    "tags:\n",
    "  - Python\n",
    "  - SEO\n",
    "---\n",
    "# Main Python Programming Guide\n",
    "Some initial content about Python programming.\n",
    "## Secondary Header\n",
    "More content here\n",
    "![Python Logo](python.png)\n",
    "\"\"\"\n",
    "\n",
    "# Mock database setup\n",
    "mock_collection = MagicMock()\n",
    "mock_db = MagicMock()\n",
    "mock_db.articles = mock_collection\n",
    "\n",
    "# Setup mock article data\n",
    "mock_article = {\n",
    "    \"metadata\": {\n",
    "        \"title\": \"Test SEO Article\",\n",
    "        \"excerpt\": \"This is a test article about python programming\"\n",
    "    },\n",
    "    \"analysis\": {\n",
    "        \"headers\": [\n",
    "            {\"type\": \"h1\", \"content\": \"Main Python Programming Guide\"},\n",
    "            {\"type\": \"h2\", \"content\": \"Secondary Header\"}\n",
    "        ],\n",
    "        \"images\": [{\"alt_text\": \"Python Logo\"}],\n",
    "        \"external_links\": {\"https://example.com\": {\"titles\": [\"Example\"]}},\n",
    "    },\n",
    "    \"focus_keyword\": \"python programming\",\n",
    "    \"related_keywords\": [\"python tutorial\"]\n",
    "}\n",
    "\n",
    "# Mock find_one to return our test article\n",
    "mock_collection.find.return_value = []  # No other articles with same keyword\n",
    "mock_collection.find_one.return_value = mock_article\n",
    "\n",
    "print(\"\\nTesting analyze_seo_rules...\")\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Create test file\n",
    "    test_file_path = os.path.join(temp_dir, \"test.md\")\n",
    "    with open(test_file_path, \"w\") as f:\n",
    "        f.write(test_content)\n",
    "    \n",
    "    store = ArticleStore(mock_db, temp_dir)\n",
    "    \n",
    "    # Run SEO analysis\n",
    "    seo_analysis = store.analyze_seo_rules(\"test.md\")\n",
    "    \n",
    "    # Test the analysis results\n",
    "    print(\"Checking SEO analysis results...\")\n",
    "    assert \"h1_count\" in seo_analysis\n",
    "    assert \"has_single_h1\" in seo_analysis\n",
    "    assert \"focus_keyword_in_excerpt\" in seo_analysis\n",
    "    assert \"keywords_in_first_10_percent\" in seo_analysis\n",
    "    assert \"has_external_links\" in seo_analysis\n",
    "    \n",
    "    # Verify specific results\n",
    "    assert seo_analysis[\"h1_count\"] == 1\n",
    "    assert seo_analysis[\"has_single_h1\"] == True\n",
    "    print(\"Checking keyword placement...\")\n",
    "    assert seo_analysis[\"focus_keyword_in_excerpt\"] == True\n",
    "    \n",
    "    print(\"Checking image optimization...\")\n",
    "    # Image alt text checks\n",
    "    assert \"keywords_in_alt_text\" in seo_analysis\n",
    "    assert isinstance(seo_analysis[\"keywords_in_alt_text\"], dict)\n",
    "    assert \"focus_keyword\" in seo_analysis[\"keywords_in_alt_text\"]\n",
    "    assert \"related_keywords\" in seo_analysis[\"keywords_in_alt_text\"]\n",
    "    \n",
    "    print(\"Checking keyword uniqueness...\")\n",
    "    # Keyword uniqueness check\n",
    "    assert \"focus_keyword_is_unique\" in seo_analysis\n",
    "    assert seo_analysis[\"focus_keyword_is_unique\"] == True\n",
    "    \n",
    "    print(\"Checking keyword statistics...\")\n",
    "    # Keyword occurrence statistics\n",
    "    assert \"focus_keyword_stats\" in seo_analysis\n",
    "    assert \"count\" in seo_analysis[\"focus_keyword_stats\"]\n",
    "    assert \"positions\" in seo_analysis[\"focus_keyword_stats\"]\n",
    "    assert isinstance(seo_analysis[\"focus_keyword_stats\"][\"positions\"], list)\n",
    "    \n",
    "    print(\"Checking external links...\")\n",
    "    # External links check\n",
    "    assert \"has_external_links\" in seo_analysis\n",
    "    \n",
    "print(\"\\nAll comprehensive SEO analysis tests passed successfully!\")   \n",
    "print(\"\\nAll SEO analysis tests passed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test environment...\n",
      "\n",
      "Testing detect_duplicate_content...\n",
      "\n",
      "Checking duplicate detection...\n",
      "\n",
      "Duplicate detection results: {'has_duplicates': True, 'similar_articles': [{'relative_path': 'similar.md', 'similarity': 0.8633879781420765, 'title': 'Similar Article'}]}\n",
      "\n",
      "Found similar content as expected\n",
      "\n",
      "All duplicate detection tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "#|test\n",
    "from unittest.mock import Mock, MagicMock\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "print(\"Setting up test environment...\")\n",
    "\n",
    "# Create original content\n",
    "original_content = \"\"\"---\n",
    "title: Original Article\n",
    "---\n",
    "# Main Content\n",
    "This is the main content of the article.\n",
    "Some specific unique content here.\n",
    "\"\"\"\n",
    "\n",
    "# Create similar content\n",
    "similar_content = \"\"\"---\n",
    "title: Similar Article\n",
    "---\n",
    "# Main Content\n",
    "This is the main content of the article.\n",
    "Some slightly different content here.\n",
    "\"\"\"\n",
    "\n",
    "# Create completely different content\n",
    "different_content = \"\"\"---\n",
    "title: Different Article\n",
    "---\n",
    "# Different Topic\n",
    "This is completely different content.\n",
    "Nothing similar here.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nTesting detect_duplicate_content...\")\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Create test files\n",
    "    original_path = os.path.join(temp_dir, \"original.md\")\n",
    "    similar_path = os.path.join(temp_dir, \"similar.md\")\n",
    "    different_path = os.path.join(temp_dir, \"different.md\")\n",
    "    \n",
    "    # Write content to files\n",
    "    for path, content in [\n",
    "        (original_path, original_content),\n",
    "        (similar_path, similar_content),\n",
    "        (different_path, different_content)\n",
    "    ]:\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    # Setup mock database\n",
    "    mock_collection = MagicMock()\n",
    "    mock_db = MagicMock()\n",
    "    mock_db.articles = mock_collection\n",
    "    \n",
    "    # Mock database responses\n",
    "    mock_collection.find.return_value = [\n",
    "        {\n",
    "            \"relative_path\": \"similar.md\",\n",
    "            \"metadata\": {\"title\": \"Similar Article\"},\n",
    "            \"file_path\": similar_path\n",
    "        },\n",
    "        {\n",
    "            \"relative_path\": \"different.md\",\n",
    "            \"metadata\": {\"title\": \"Different Article\"},\n",
    "            \"file_path\": different_path\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create store and test\n",
    "    store = ArticleStore(mock_db, temp_dir)\n",
    "    \n",
    "    # Test duplicate detection\n",
    "    print(\"\\nChecking duplicate detection...\")\n",
    "    results = store.detect_duplicate_content(\"original.md\", similarity_threshold=0.8)\n",
    "    \n",
    "    # Debug output\n",
    "    print(f\"\\nDuplicate detection results: {results}\")\n",
    "    \n",
    "    # Assertions\n",
    "    assert \"has_duplicates\" in results\n",
    "    assert \"similar_articles\" in results\n",
    "    assert isinstance(results[\"similar_articles\"], list)\n",
    "    \n",
    "    # Check similar content was detected\n",
    "    if results[\"has_duplicates\"]:\n",
    "        assert any(article[\"similarity\"] >= 0.8 for article in results[\"similar_articles\"])\n",
    "        print(\"\\nFound similar content as expected\")\n",
    "    \n",
    "print(\"\\nAll duplicate detection tests passed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test environment...\n",
      "\n",
      "Testing keyword cannibalization...\n",
      "\n",
      "Cannibalization results: {'has_cannibalization': True, 'keyword': 'python programming', 'count': 2, 'articles': [{'relative_path': 'article1.md', 'title': 'Python Guide 1', 'publish_date': '2024-01-01', 'keyword_type': 'focus', 'density': 5.0}, {'relative_path': 'article2.md', 'title': 'Python Guide 2', 'publish_date': '2024-01-02', 'keyword_type': 'focus', 'density': 5.0}]}\n",
      "\n",
      "All cannibalization tests passed!\n"
     ]
    }
   ],
   "source": [
    "#|test\n",
    "from unittest.mock import Mock, MagicMock\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "print(\"Setting up test environment...\")\n",
    "\n",
    "# Create multiple articles with same keyword\n",
    "articles_content = {\n",
    "    \"article1.md\": \"\"\"---\n",
    "title: Python Guide 1\n",
    "---\n",
    "# Python Programming\n",
    "This is a guide about Python programming.\n",
    "\"\"\",\n",
    "    \"article2.md\": \"\"\"---\n",
    "title: Python Guide 2\n",
    "---\n",
    "# Python Tutorial\n",
    "Another article about Python programming.\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "print(\"\\nTesting keyword cannibalization...\")\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Setup mock database\n",
    "    mock_collection = MagicMock()\n",
    "    mock_db = MagicMock()\n",
    "    mock_db.articles = mock_collection\n",
    "    \n",
    "    # Mock database responses\n",
    "    mock_collection.find.return_value = [\n",
    "        {\n",
    "            \"relative_path\": \"article1.md\",\n",
    "            \"metadata\": {\n",
    "                \"title\": \"Python Guide 1\",\n",
    "                \"publishDate\": \"2024-01-01\"\n",
    "            },\n",
    "            \"focus_keyword\": \"python programming\"\n",
    "        },\n",
    "        {\n",
    "            \"relative_path\": \"article2.md\",\n",
    "            \"metadata\": {\n",
    "                \"title\": \"Python Guide 2\",\n",
    "                \"publishDate\": \"2024-01-02\"\n",
    "            },\n",
    "            \"focus_keyword\": \"python programming\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    store = ArticleStore(mock_db, temp_dir)\n",
    "    \n",
    "    # Mock the calculate_keyword_density method\n",
    "    store.calculate_keyword_density = MagicMock(return_value={\n",
    "        \"focus_keyword\": {\n",
    "            \"keyword\": \"python programming\",\n",
    "            \"count\": 3,\n",
    "            \"density\": 5.0\n",
    "        },\n",
    "        \"related_keywords\": []\n",
    "    })\n",
    "    \n",
    "    # Test cannibalization analysis\n",
    "    results = store.analyze_keyword_cannibalization(\"python programming\")\n",
    "    \n",
    "    # Debug output\n",
    "    print(f\"\\nCannibalization results: {results}\")\n",
    "    \n",
    "    # Assertions\n",
    "    assert \"has_cannibalization\" in results\n",
    "    assert results[\"has_cannibalization\"] == True\n",
    "    assert \"count\" in results\n",
    "    assert results[\"count\"] >= 2\n",
    "    assert \"articles\" in results\n",
    "    assert len(results[\"articles\"]) >= 2\n",
    "    \n",
    "print(\"\\nAll cannibalization tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test environment...\n",
      "\n",
      "Testing analyze_content_groups...\n",
      "\n",
      "Content groups analysis results: {'total_articles': 3, 'groups': [{'main_article': {'title': 'Original Article', 'path': 'article1.md'}, 'similar_articles': [{'title': 'Similar Article', 'path': 'article2.md', 'similarity': 0.9932885906040269}], 'common_content': ['Some shared information here.', 'This is some common content that appears in multiple articles.', '# Common Content'], 'unique_content': ['Plus some unique content for article 1.']}], 'unique_articles': 1, 'duplicate_groups': 1}\n",
      "\n",
      "All content groups analysis tests passed!\n"
     ]
    }
   ],
   "source": [
    "#|test\n",
    "from unittest.mock import Mock, MagicMock\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "print(\"Setting up test environment...\")\n",
    "\n",
    "# Create test articles with varying similarity\n",
    "articles_content = {\n",
    "    \"article1.md\": \"\"\"---\n",
    "title: Original Article\n",
    "---\n",
    "# Common Content\n",
    "This is some common content that appears in multiple articles.\n",
    "Some shared information here.\n",
    "Plus some unique content for article 1.\n",
    "\"\"\",\n",
    "\n",
    "    \"article2.md\": \"\"\"---\n",
    "title: Similar Article\n",
    "---\n",
    "# Common Content\n",
    "This is some common content that appears in multiple articles.\n",
    "Some shared information here.\n",
    "Plus some unique content for article 2.\n",
    "\"\"\",\n",
    "\n",
    "    \"article3.md\": \"\"\"---\n",
    "title: Different Article\n",
    "---\n",
    "# Unique Topic\n",
    "This is completely different content.\n",
    "Nothing similar here.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"\\nTesting analyze_content_groups...\")\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Create test files\n",
    "    for filename, content in articles_content.items():\n",
    "        with open(os.path.join(temp_dir, filename), \"w\") as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    # Setup mock database\n",
    "    mock_collection = MagicMock()\n",
    "    mock_db = MagicMock()\n",
    "    mock_db.articles = mock_collection\n",
    "    \n",
    "    # Mock database find to return our test articles\n",
    "    mock_collection.find.return_value = [\n",
    "        {\n",
    "            \"relative_path\": filename,\n",
    "            \"metadata\": {\"title\": content.split(\"\\n\")[1].replace(\"title: \", \"\")},\n",
    "        }\n",
    "        for filename, content in articles_content.items()\n",
    "    ]\n",
    "    \n",
    "    store = ArticleStore(mock_db, temp_dir)\n",
    "    \n",
    "    # Test content groups analysis\n",
    "    results = store.analyze_content_groups(similarity_threshold=0.7)\n",
    "    \n",
    "    # Debug output\n",
    "    print(f\"\\nContent groups analysis results: {results}\")\n",
    "    \n",
    "    # Assertions\n",
    "    assert \"total_articles\" in results\n",
    "    assert results[\"total_articles\"] == 3\n",
    "    assert \"groups\" in results\n",
    "    assert \"unique_articles\" in results\n",
    "    assert \"duplicate_groups\" in results\n",
    "    \n",
    "    # Check group structure\n",
    "    if results[\"groups\"]:\n",
    "        group = results[\"groups\"][0]\n",
    "        assert \"main_article\" in group\n",
    "        assert \"similar_articles\" in group\n",
    "        assert \"common_content\" in group\n",
    "        assert \"unique_content\" in group\n",
    "    \n",
    "print(\"\\nAll content groups analysis tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
