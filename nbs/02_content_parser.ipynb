{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b76427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp content_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f44ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea1f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "from fastcore.test import test_eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9764a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def parse_metadata(content: str) -> Dict:\n",
    "    \"\"\"Extract metadata from content frontmatter\"\"\"\n",
    "    yaml_section = content.split(\"---\")[1]\n",
    "    return yaml.safe_load(yaml_section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40b9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def parse_notebook_metadata(content: str) -> Dict:\n",
    "    \"\"\"Extract metadata from Jupyter notebook\"\"\"\n",
    "    import json\n",
    "\n",
    "    notebook = json.loads(content)\n",
    "\n",
    "    # Check first cell for YAML frontmatter\n",
    "    if notebook.get(\"cells\"):\n",
    "        first_cell = notebook[\"cells\"][0]\n",
    "        if first_cell.get(\"cell_type\") == \"markdown\":\n",
    "            source = \"\".join(first_cell.get(\"source\", []))\n",
    "            if source.startswith(\"---\"):\n",
    "                return parse_metadata(source)\n",
    "\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "117726c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# | test\n",
    "# Test Parse metadat\n",
    "from nbdev.qmd import meta\n",
    "with open(\"../sample/example.md\", \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "metadata = parse_metadata(content)\n",
    "\n",
    "#content is .md \n",
    "# with open(\"../sample/example.md\", \"r\") as f:\n",
    "#     content = f.read()\n",
    "\n",
    "#content is .ipynb\n",
    "with open(\"../sample/design_questions.ipynb\", \"r\") as f:\n",
    "    content = f.read()\n",
    "metadata = parse_notebook_metadata(content)\n",
    "print(metadata)\n",
    "# metadata = parse_metadata(content)\n",
    "# test_eq(metadata[\"title\"], \"Kareem Elkhateb SEO Trend Example\")\n",
    "# test_eq(str(metadata[\"publishDate\"]), \"2024-01-27\")\n",
    "# test_eq(metadata[\"tags\"], [\"Astrojs\", \"Rust\", \"C++\", \"C#\", \"Camel_Space\", \"Horse Case\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ad6178a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Design Basic SEO \\n',\n",
       "    '\\n',\n",
       "    '### Design Ideas\\n',\n",
       "    '\\n',\n",
       "    '1. Collect all URls and then apply the separeted external and internal \\n',\n",
       "    '\\n',\n",
       "    '2. How to deal with different types of urls \\n',\n",
       "    '   1. images\\n',\n",
       "    '   2. Videos\\n',\n",
       "    '   3. Markdown internal photo\\n']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['\\n',\n",
       "    '### Design Questions\\n',\n",
       "    '\\n',\n",
       "    '1. How to read files and work with them efficiently to handle issues of \\n',\n",
       "    '   1. Get out of memory \\n',\n",
       "    '   2. Slow execution \\n',\n",
       "    '\\n',\n",
       "    '2. Should I remove the frontmatter while processing the data and make in a different API ! \\n',\n",
       "    '\\n',\n",
       "    '3. Should i design the API to Work the Md content or read the file content with file path! ']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Design Keywords\\n',\n",
       "    '\\n',\n",
       "    '### Design Concepts\\n',\n",
       "    '\\n',\n",
       "    '1. Should I Store them mainly into a SQL database! and make use python functions as  a helpers to store metadata into the DB?? \\n',\n",
       "    '2. I should find a better way to store the keywords associated with the my files documents\\n',\n",
       "    '\\n',\n",
       "    '#### Focus keywords Attributes! \\n',\n",
       "    '\\n',\n",
       "    '1. Adding \\n',\n",
       "    '2. Updating Keyword\\n',\n",
       "    '3. Adding and upadting the search volume \\n',\n",
       "    '4. Adding and Updating the Ranking \\n',\n",
       "    '5. Primary Keyword\\n',\n",
       "    '   1. Adding Primary Focus keyword\\n',\n",
       "    '   2. Change Primary Focus Keyword \\n',\n",
       "    '6. Get Length of the keywords']},\n",
       "  {'cell_type': 'markdown', 'metadata': {}, 'source': []}],\n",
       " 'metadata': {'language_info': {'name': 'python'}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 2}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "notebook = json.loads(content)\n",
    "notebook \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d453e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_notebook_content(content:str, is_quarto:bool=False)->str:\n",
    "    source= \"\"\n",
    "    if notebook.get(\"cells\"):\n",
    "        first_cell = 0\n",
    "        for cell in notebook[\"cells\"]:\n",
    "            if cell.get(\"cell_type\") == \"markdown\" and first_cell == 0:\n",
    "                if source.startswith(\"---\"):\n",
    "                    first_cell=1\n",
    "                    continue \n",
    "            if cell.get(\"cell_type\") == \"markdown\":\n",
    "                source += \"\".join(cell.get(\"source\", []))\n",
    "            if is_quarto:\n",
    "                if cell.get(\"cell_type\") == \"code\":\n",
    "                    if cell.get(\"source\").contains(\"#| echo: false\") or cell.get(\"source\").contains(\"#| include: false\"):\n",
    "                        continue\n",
    "                    source += \"\".join(cell.get(\"source\", []))\n",
    "\n",
    "    return source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9460e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def remove_metadata(content: str) -> str:\n",
    "    \"\"\"Remove frontmatter from content\"\"\"\n",
    "    end = content.find(\"---\", 3)\n",
    "    return content[end + 3 :].strip() if end != -1 else content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a5f68e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"## Design Basic SEO \\\\n\",\\n    \"\\\\n\",\\n    \"### Design Ideas\\\\n\",\\n    \"\\\\n\",\\n    \"1. Collect all URls and then apply the separeted external and internal \\\\n\",\\n    \"\\\\n\",\\n    \"2. How to deal with different types of urls \\\\n\",\\n    \"   1. images\\\\n\",\\n    \"   2. Videos\\\\n\",\\n    \"   3. Markdown internal photo\\\\n\",\\n    \"### Design Questions\\\\n\",\\n    \"\\\\n\",\\n    \"1. How to read files and work with them efficiently to handle issues of \\\\n\",\\n    \"   1. Get out of memory \\\\n\",\\n    \"   2. Slow execution \\\\n\",\\n    \"\\\\n\",\\n    \"2. Should I remove the frontmatter while processing the data and make in a different API ! \\\\n\",\\n    \"\\\\n\",\\n    \"3. Should i design the API to Work the Md content or read the file content with file path! \"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"## Design Keywords\\\\n\",\\n    \"\\\\n\",\\n    \"### Design Concepts\\\\n\",\\n    \"\\\\n\",\\n    \"1. Should I Store them mainly into a SQL database! and make use python functions as  a helpers to store metadata into the DB?? \\\\n\",\\n    \"2. I should find a better way to store the keywords associated with the my files documents\\\\n\",\\n    \"\\\\n\",\\n    \"#### Focus keywords Attributes! \\\\n\",\\n    \"\\\\n\",\\n    \"1. Adding \\\\n\",\\n    \"2. Updating Keyword\\\\n\",\\n    \"3. Adding and upadting the search volume \\\\n\",\\n    \"4. Adding and Updating the Ranking \\\\n\",\\n    \"5. Primary Keyword\\\\n\",\\n    \"   1. Adding Primary Focus keyword\\\\n\",\\n    \"   2. Change Primary Focus Keyword \\\\n\",\\n    \"6. Get Length of the keywords\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"language_info\": {\\n   \"name\": \"python\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | test\n",
    "# Test Remove metadata\n",
    "content = remove_metadata(content)\n",
    "content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9050c6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"## Design Basic SEO \\\\n\",\\n    \"\\\\n\",\\n    \"### Design Ideas\\\\n\",\\n    \"\\\\n\",\\n    \"1. Collect all URls and then apply the separeted external and internal \\\\n\",\\n    \"\\\\n\",\\n    \"2. How to deal with different types of urls \\\\n\",\\n    \"   1. images\\\\n\",\\n    \"   2. Videos\\\\n\",\\n    \"   3. Markdown internal photo\\\\n\",\\n    \"### Design Questions\\\\n\",\\n    \"\\\\n\",\\n    \"1. How to read files and work with them efficiently to handle issues of \\\\n\",\\n    \"   1. Get out of memory \\\\n\",\\n    \"   2. Slow execution \\\\n\",\\n    \"\\\\n\",\\n    \"2. Should I remove the frontmatter while processing the data and make in a different API ! \\\\n\",\\n    \"\\\\n\",\\n    \"3. Should i design the API to Work the Md content or read the file content with file path! \"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"## Design Keywords\\\\n\",\\n    \"\\\\n\",\\n    \"### Design Concepts\\\\n\",\\n    \"\\\\n\",\\n    \"1. Should I Store them mainly into a SQL database! and make use python functions as  a helpers to store metadata into the DB?? \\\\n\",\\n    \"2. I should find a better way to store the keywords associated with the my files documents\\\\n\",\\n    \"\\\\n\",\\n    \"#### Focus keywords Attributes! \\\\n\",\\n    \"\\\\n\",\\n    \"1. Adding \\\\n\",\\n    \"2. Updating Keyword\\\\n\",\\n    \"3. Adding and upadting the search volume \\\\n\",\\n    \"4. Adding and Updating the Ranking \\\\n\",\\n    \"5. Primary Keyword\\\\n\",\\n    \"   1. Adding Primary Focus keyword\\\\n\",\\n    \"   2. Change Primary Focus Keyword \\\\n\",\\n    \"6. Get Length of the keywords\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"language_info\": {\\n   \"name\": \"python\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "657150ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_headers(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Extract all headers with metadata\"\"\"\n",
    "    headings = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            line = line.strip()\n",
    "            for level in range(1, 7):\n",
    "                prefix = \"#\" * level + \" \"\n",
    "                if line.startswith(prefix):\n",
    "                    content = line.strip(\"#\").strip()\n",
    "                    headings.append(\n",
    "                        {\n",
    "                            \"type\": f\"h{level}\",\n",
    "                            \"line_number\": line_number,\n",
    "                            \"content\": content,\n",
    "                            \"length\": len(content),\n",
    "                        }\n",
    "                    )\n",
    "                    break\n",
    "    return headings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecd80692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "headers = extract_headers(\"../sample/example.md\")\n",
    "test_eq(len([h for h in headers if h[\"type\"] == \"h1\"]), 2)\n",
    "test_eq(headers[0][\"content\"], \"This is me Kareem\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09640fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def check_title_length(title: str) -> Dict:\n",
    "    length = len(title)\n",
    "    return {\"length\": length, \"optimal_lenth\": 50 <= length <= 60}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4661124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def check_desc_length(description: str) -> Dict:\n",
    "    length = len(description)\n",
    "    return {\"length\": length, \"optimal_lenth\": 150 <= length <= 160}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc169660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def check_content_length(content: str) -> Dict:\n",
    "    \"\"\"Count words in content\"\"\"\n",
    "    words = len(content.split())\n",
    "    return {\"word_count\": words, \"is_sufficient\": words >= 300}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2fb225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'length': 33, 'optimal_lenth': False}\n",
      "{'length': 146, 'optimal_lenth': False}\n",
      "{'is_sufficient': False, 'word_count': 124}\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(check_title_length(metadata[\"title\"]))\n",
    "pprint(check_desc_length(metadata[\"excertp\"]))\n",
    "pprint(check_content_length(content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc56687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_links(content: str) -> Dict[str, Dict]:\n",
    "    \"\"\"Extract all links with metadata\"\"\"\n",
    "    links = {}\n",
    "    lines = content.split(\"\\n\")\n",
    "    for line_number, line in enumerate(lines, start=1):\n",
    "        for match in re.finditer(r\"\\[(.*?)\\]\\((.*?)\\)\", line):\n",
    "            title, url = match.groups()\n",
    "            if url not in links:\n",
    "                links[url] = {\"titles\": [], \"lines\": []}\n",
    "            links[url][\"titles\"].append(title)\n",
    "            links[url][\"lines\"].append(line_number)\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ba1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "links = extract_links(content)\n",
    "test_eq(\"https://emdadelgaz.com\" in links, True)\n",
    "test_eq(\"https://awazly.com/\" in links, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "902e3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_images(content: str) -> List[Dict]:\n",
    "    \"\"\"Extract images with alt text\"\"\"\n",
    "    matches = re.findall(r\"\\!\\[(.*?)\\]\\((.*?)\\)\", content)\n",
    "    return [{\"alt_text\": alt, \"url\": url} for alt, url in matches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a91e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def imgs_missing_alts(images: List[Dict]) -> List[str]:\n",
    "    \"\"\"Return URLs of images missing alt text\"\"\"\n",
    "    return [img[\"url\"] for img in images if not img.get(\"alt_text\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "409402ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "images = extract_images(content)\n",
    "test_eq(len(images), 1)\n",
    "test_eq(images[0][\"alt_text\"], \"Iron man photo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e2970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def filter_internal_links(urls: List[str], domain: str) -> List[str]:\n",
    "    \"\"\"Filter for internal links (excludes images)\"\"\"\n",
    "    image_exts = (\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".svg\", \".webp\")\n",
    "    internal = []\n",
    "\n",
    "    for url in urls:\n",
    "        # Skip images\n",
    "        if url.lower().endswith(image_exts):\n",
    "            continue\n",
    "        # Skip anchors\n",
    "        if url.startswith(\"#\"):\n",
    "            continue\n",
    "        # Relative paths are internal\n",
    "        if not url.startswith(\"http\"):\n",
    "            internal.append(url)\n",
    "        # Same domain\n",
    "        elif urlparse(url).netloc == domain:\n",
    "            internal.append(url)\n",
    "\n",
    "    return internal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d120d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def filter_external_links(urls: List[str], domain: str) -> List[str]:\n",
    "    \"\"\"Filter for external links only\"\"\"\n",
    "    image_exts = (\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".svg\", \".webp\")\n",
    "    internal = filter_internal_links(urls, domain)\n",
    "\n",
    "    return [\n",
    "        url\n",
    "        for url in urls\n",
    "        if url not in internal  # Exclude internal\n",
    "        and not url.lower().endswith(image_exts)\n",
    "    ]  # Exclude images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67e05771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text by removing extra whitespace\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd856f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def detect_phone_numbers(text: str) -> List[str]:\n",
    "    \"\"\"Extract phone numbers from text\"\"\"\n",
    "    phone_regex = re.compile(r\"(\\+\\d{1,3})?\\s*?(\\d{3})\\s*?(\\d{3})\\s*?(\\d{3,4})\")\n",
    "    groups = phone_regex.findall(text)\n",
    "    return [\"\".join(g) for g in groups]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ccbfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "phones = detect_phone_numbers(content)\n",
    "test_eq(\"+966503139675\" in phones, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b08ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate similarity ratio between two texts\"\"\"\n",
    "    from difflib import SequenceMatcher\n",
    "\n",
    "    return SequenceMatcher(None, text1, text2).ratio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21534583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_file_paths(pattern: str) -> List[str]:\n",
    "    \"\"\"Get file paths matching pattern\"\"\"\n",
    "    import glob\n",
    "\n",
    "    return glob.glob(pattern, recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14b3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_file_name(file_path: str) -> str:\n",
    "    \"\"\"Extract filename without extension from path\"\"\"\n",
    "    return Path(file_path).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4094fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_markdown_files(directory: str) -> List[str]:\n",
    "    \"\"\"Get all markdown filenames (without extension) from directory\"\"\"\n",
    "    import os\n",
    "\n",
    "    return [\n",
    "        f.replace(\".md\", \"\")\n",
    "        for f in os.listdir(directory)\n",
    "        if f.endswith(\".md\") and f != \".obsidian\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6913bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def arabic_to_slug(text: str) -> str:\n",
    "    \"\"\"Convert Arabic text to URL-friendly slug\"\"\"\n",
    "    char_map = {\n",
    "        \"ا\": \"a\",\n",
    "        \"ب\": \"b\",\n",
    "        \"ت\": \"t\",\n",
    "        \"ث\": \"th\",\n",
    "        \"ج\": \"j\",\n",
    "        \"ح\": \"h\",\n",
    "        \"خ\": \"kh\",\n",
    "        \"د\": \"d\",\n",
    "        \"ذ\": \"th\",\n",
    "        \"ر\": \"r\",\n",
    "        \"ز\": \"z\",\n",
    "        \"س\": \"s\",\n",
    "        \"ش\": \"sh\",\n",
    "        \"ص\": \"s\",\n",
    "        \"ض\": \"d\",\n",
    "        \"ط\": \"t\",\n",
    "        \"ظ\": \"z\",\n",
    "        \"ع\": \"\",\n",
    "        \"غ\": \"gh\",\n",
    "        \"ف\": \"f\",\n",
    "        \"ق\": \"q\",\n",
    "        \"ك\": \"k\",\n",
    "        \"ل\": \"l\",\n",
    "        \"م\": \"m\",\n",
    "        \"ن\": \"n\",\n",
    "        \"ه\": \"h\",\n",
    "        \"و\": \"w\",\n",
    "        \"ي\": \"y\",\n",
    "        \"ة\": \"h\",\n",
    "        \" \": \"-\",\n",
    "    }\n",
    "\n",
    "    slug = \"\".join(char_map.get(c, c) for c in text.strip().lower())\n",
    "    while \"--\" in slug:\n",
    "        slug = slug.replace(\"--\", \"-\")\n",
    "    return slug.strip(\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e334816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def map_files_to_slugs(directory: str) -> Dict[str, str]:\n",
    "    \"\"\"Map markdown filenames to URL slugs\"\"\"\n",
    "    files = get_markdown_files(directory)\n",
    "    return {filename: arabic_to_slug(filename) for filename in files}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
