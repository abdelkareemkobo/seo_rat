{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp seo_site_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c004a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from sqlmodel import Session, select\n",
    "from seo_rat.article import Article\n",
    "from seo_rat.content_parser import remove_metadata, normalize_text, calculate_similarity\n",
    "from seo_rat.seo_content_analysis import calculate_keyword_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebe122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def detect_duplicate_content(\n",
    "    session: Session, file_path: str, similarity_threshold: float = 0.8\n",
    ") -> dict:\n",
    "    \"\"\"Find articles in the DB with content similar to the given file\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        current_content = normalize_text(remove_metadata(f.read()))\n",
    "\n",
    "    articles = session.exec(select(Article)).all()\n",
    "    similar = []\n",
    "\n",
    "    for article in articles:\n",
    "        if article.file_path == file_path:\n",
    "            continue\n",
    "        with open(article.file_path, \"r\") as f:\n",
    "            other_content = normalize_text(remove_metadata(f.read()))\n",
    "\n",
    "        similarity = calculate_similarity(current_content, other_content)\n",
    "        if similarity >= similarity_threshold:\n",
    "            similar.append({\"file_path\": article.file_path, \"similarity\": similarity})\n",
    "\n",
    "    return {\"has_duplicates\": len(similar) > 0, \"similar_articles\": similar}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead01c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def analyze_keyword_cannibalization(session: Session, keyword: str) -> dict:\n",
    "    \"\"\"Find articles competing for the same focus keyword\"\"\"\n",
    "    articles = session.exec(\n",
    "        select(Article).where(Article.focus_keyword == keyword)\n",
    "    ).all()\n",
    "\n",
    "    if len(articles) <= 1:\n",
    "        return {\n",
    "            \"has_cannibalization\": False,\n",
    "            \"keyword\": keyword,\n",
    "            \"count\": len(articles),\n",
    "        }\n",
    "\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        with open(article.file_path, \"r\") as f:\n",
    "            content = remove_metadata(f.read())\n",
    "        density = calculate_keyword_density(content, keyword)\n",
    "        results.append(\n",
    "            {\n",
    "                \"file_path\": article.file_path,\n",
    "                \"density\": density[\"density\"],\n",
    "                \"count\": density[\"count\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"has_cannibalization\": True,\n",
    "        \"keyword\": keyword,\n",
    "        \"count\": len(articles),\n",
    "        \"articles\": results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def analyze_content_groups(session: Session, similarity_threshold: float = 0.8) -> dict:\n",
    "    \"\"\"Group similar articles together across the whole site\"\"\"\n",
    "    articles = session.exec(select(Article)).all()\n",
    "    groups = []\n",
    "    processed: set[int] = set()\n",
    "\n",
    "    for article in articles:\n",
    "        if article.id in processed:\n",
    "            continue\n",
    "\n",
    "        with open(article.file_path, \"r\") as f:\n",
    "            main_content = normalize_text(remove_metadata(f.read()))\n",
    "\n",
    "        group: dict = {\"main_article\": article.file_path, \"similar_articles\": []}\n",
    "\n",
    "        for other in articles:\n",
    "            if other.id == article.id or other.id in processed:\n",
    "                continue\n",
    "\n",
    "            with open(other.file_path, \"r\") as f:\n",
    "                other_content = normalize_text(remove_metadata(f.read()))\n",
    "\n",
    "            similarity = calculate_similarity(main_content, other_content)\n",
    "            if similarity >= similarity_threshold:\n",
    "                group[\"similar_articles\"].append(\n",
    "                    {\"file_path\": other.file_path, \"similarity\": similarity}\n",
    "                )\n",
    "                processed.add(other.id)\n",
    "\n",
    "        if group[\"similar_articles\"]:\n",
    "            groups.append(group)\n",
    "            processed.add(article.id)\n",
    "\n",
    "    return {\n",
    "        \"total_articles\": len(articles),\n",
    "        \"groups\": groups,\n",
    "        \"duplicate_groups\": len(groups),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "from fastcore.test import test_eq\n",
    "from pprint import pprint\n",
    "from sqlmodel import create_engine, Session, SQLModel\n",
    "from seo_rat.models import Website\n",
    "from seo_rat.article import Article, insert_article\n",
    "import tempfile, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "from pathlib import Path\n",
    "\n",
    "sample_dir = Path(\"sample\")\n",
    "if not sample_dir.exists():\n",
    "    sample_dir = Path(\"../sample\")\n",
    "\n",
    "with open(sample_dir / \"example.md\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "SQLModel.metadata.create_all(engine)\n",
    "\n",
    "with Session(engine) as session:\n",
    "    website = Website(url=\"https://test.com\", name=\"Test\", lang=\"en\")\n",
    "    session.add(website)\n",
    "    session.commit()\n",
    "    session.refresh(website)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".md\", delete=False) as f1:\n",
    "        f1.write(content)\n",
    "        path1 = f1.name\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".md\", delete=False) as f2:\n",
    "        f2.write(content)\n",
    "        path2 = f2.name\n",
    "\n",
    "    article1 = insert_article(session, website.id, path1, focus_keyword=\"Kareem\")\n",
    "    article2 = insert_article(session, website.id, path2, focus_keyword=\"Kareem\")\n",
    "\n",
    "    # detect_duplicate_content\n",
    "    dup_result = detect_duplicate_content(session, path1)\n",
    "    test_eq(dup_result[\"has_duplicates\"], True)\n",
    "    pprint(dup_result)\n",
    "\n",
    "    # analyze_keyword_cannibalization\n",
    "    cannibal_result = analyze_keyword_cannibalization(session, \"Kareem\")\n",
    "    test_eq(cannibal_result[\"has_cannibalization\"], True)\n",
    "    test_eq(cannibal_result[\"count\"], 2)\n",
    "    pprint(cannibal_result)\n",
    "\n",
    "    # analyze_content_groups\n",
    "    groups_result = analyze_content_groups(session, 0.8)\n",
    "    test_eq(groups_result[\"duplicate_groups\"] >= 1, True)\n",
    "    pprint(groups_result)\n",
    "\n",
    "    os.unlink(path1)\n",
    "    os.unlink(path2)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
