{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fed96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp seo_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b261f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Dict, List, Tuple\n",
    "from urllib.parse import urlparse\n",
    "from sqlmodel import Session, select\n",
    "from seo_rat.article import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6303d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def calculate_keyword_density(content: str, keyword: str) -> Dict:\n",
    "    \"\"\"Calculate keyword density and positions\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    keyword_lower = keyword.lower()\n",
    "\n",
    "    # Find all positions\n",
    "    positions = []\n",
    "    pos = 0\n",
    "    while (pos := content_lower.find(keyword_lower, pos)) != -1:\n",
    "        positions.append(pos)\n",
    "        pos += 1\n",
    "\n",
    "    # Calculate density\n",
    "    total_words = len(content.split())\n",
    "    count = len(positions)\n",
    "    density = (count / total_words * 100) if total_words > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"keyword\": keyword,\n",
    "        \"count\": count,\n",
    "        \"density\": density,\n",
    "        \"positions\": positions,\n",
    "    }\n",
    "\n",
    "\n",
    "# | export\n",
    "def check_h1_count(headers: List[Dict]) -> Dict:\n",
    "    \"\"\"Check H1 heading count\"\"\"\n",
    "    h1s = [h for h in headers if h[\"type\"] == \"h1\"]\n",
    "    return {\n",
    "        \"h1_count\": len(h1s),\n",
    "        \"has_single_h1\": len(h1s) == 1,\n",
    "        \"h1_contents\": [h[\"content\"] for h in h1s],\n",
    "    }\n",
    "\n",
    "\n",
    "# | export\n",
    "def keyword_in_first_section(content: str, keyword: str, percent: int = 10) -> bool:\n",
    "    \"\"\"Check if keyword appears in first X% of content\"\"\"\n",
    "    section_length = int(len(content) * percent / 100)\n",
    "    return keyword.lower() in content[:section_length].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7778c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def keyword_in_metadata(metadata: Dict, keyword: str) -> Dict:\n",
    "    \"\"\"Check if keyword is in title, excerpt, description\"\"\"\n",
    "    return {\n",
    "        \"in_title\": keyword.lower() in str(metadata.get(\"title\", \"\")).lower(),\n",
    "        \"in_excerpt\": keyword.lower() in str(metadata.get(\"excerpt\", \"\")).lower(),\n",
    "        \"in_description\": keyword.lower()\n",
    "        in str(metadata.get(\"description\", \"\")).lower(),\n",
    "    }\n",
    "\n",
    "\n",
    "# | export\n",
    "def keyword_in_alt_texts(images: List[Dict], keyword: str) -> bool:\n",
    "    \"\"\"Check if keyword appears in any image alt text\"\"\"\n",
    "    return any(keyword.lower() in img[\"alt_text\"].lower() for img in images)\n",
    "\n",
    "\n",
    "# | export\n",
    "def analyze_header_distribution(headers: List[Dict]) -> Dict:\n",
    "    \"\"\"Analyze header hierarchy distribution\"\"\"\n",
    "    distribution = {}\n",
    "    for h in headers:\n",
    "        h_type = h[\"type\"]\n",
    "        distribution[h_type] = distribution.get(h_type, 0) + 1\n",
    "\n",
    "    total = len(headers)\n",
    "    percentages = {\n",
    "        k: (v / total * 100) if total > 0 else 0 for k, v in distribution.items()\n",
    "    }\n",
    "\n",
    "    return {\"counts\": distribution, \"percentages\": percentages}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6453b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def detect_duplicate_content(\n",
    "    session: Session, file_path: str, similarity_threshold: float = 0.8\n",
    ") -> Dict:\n",
    "    \"\"\"Find similar articles by comparing content\"\"\"\n",
    "    from seo_rat.content_parser import (\n",
    "        remove_metadata,\n",
    "        normalize_text,\n",
    "        calculate_similarity,\n",
    "    )\n",
    "\n",
    "    # Read current article\n",
    "    with open(file_path, \"r\") as f:\n",
    "        current_content = normalize_text(remove_metadata(f.read()))\n",
    "\n",
    "    # Get all other articles\n",
    "    articles = session.exec(select(Article)).all()\n",
    "    similar = []\n",
    "\n",
    "    for article in articles:\n",
    "        if article.file_path == file_path:\n",
    "            continue\n",
    "\n",
    "        with open(article.file_path, \"r\") as f:\n",
    "            other_content = normalize_text(remove_metadata(f.read()))\n",
    "\n",
    "        similarity = calculate_similarity(current_content, other_content)\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            similar.append({\"file_path\": article.file_path, \"similarity\": similarity})\n",
    "\n",
    "    return {\"has_duplicates\": len(similar) > 0, \"similar_articles\": similar}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37218e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def analyze_keyword_cannibalization(session: Session, keyword: str) -> Dict:\n",
    "    \"\"\"Find articles competing for same keyword\"\"\"\n",
    "    from seo_rat.content_parser import remove_metadata, calculate_keyword_density\n",
    "\n",
    "    articles = session.exec(\n",
    "        select(Article).where(Article.focus_keyword == keyword)\n",
    "    ).all()\n",
    "\n",
    "    if len(articles) <= 1:\n",
    "        return {\n",
    "            \"has_cannibalization\": False,\n",
    "            \"keyword\": keyword,\n",
    "            \"count\": len(articles),\n",
    "        }\n",
    "\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        with open(article.file_path, \"r\") as f:\n",
    "            content = remove_metadata(f.read())\n",
    "\n",
    "        density = calculate_keyword_density(content, keyword)\n",
    "        results.append(\n",
    "            {\n",
    "                \"file_path\": article.file_path,\n",
    "                \"density\": density[\"density\"],\n",
    "                \"count\": density[\"count\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"has_cannibalization\": True,\n",
    "        \"keyword\": keyword,\n",
    "        \"count\": len(articles),\n",
    "        \"articles\": results,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c5ed414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def analyze_content_groups(session: Session, similarity_threshold: float = 0.8) -> Dict:\n",
    "    \"\"\"Group similar articles together\"\"\"\n",
    "    from seo_rat.content_parser import (\n",
    "        remove_metadata,\n",
    "        normalize_text,\n",
    "        calculate_similarity,\n",
    "    )\n",
    "\n",
    "    articles = session.exec(select(Article)).all()\n",
    "    groups = []\n",
    "    processed = set()\n",
    "\n",
    "    for article in articles:\n",
    "        if article.id in processed:\n",
    "            continue\n",
    "\n",
    "        with open(article.file_path, \"r\") as f:\n",
    "            main_content = normalize_text(remove_metadata(f.read()))\n",
    "\n",
    "        group = {\"main_article\": article.file_path, \"similar_articles\": []}\n",
    "\n",
    "        for other in articles:\n",
    "            if other.id == article.id or other.id in processed:\n",
    "                continue\n",
    "\n",
    "            with open(other.file_path, \"r\") as f:\n",
    "                other_content = normalize_text(remove_metadata(f.read()))\n",
    "\n",
    "            similarity = calculate_similarity(main_content, other_content)\n",
    "\n",
    "            if similarity >= similarity_threshold:\n",
    "                group[\"similar_articles\"].append(\n",
    "                    {\"file_path\": other.file_path, \"similarity\": similarity}\n",
    "                )\n",
    "                processed.add(other.id)\n",
    "\n",
    "        if group[\"similar_articles\"]:\n",
    "            groups.append(group)\n",
    "            processed.add(article.id)\n",
    "\n",
    "    return {\n",
    "        \"total_articles\": len(articles),\n",
    "        \"groups\": groups,\n",
    "        \"duplicate_groups\": len(groups),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7af50f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "def get_num_heads(h_elements):\n",
    "    \"\"\"\n",
    "    Return A list continas the length of each heading\n",
    "\n",
    "    Takes the heading info from `get_heads_info`\n",
    "    \"\"\"\n",
    "    #! Update this to work with the new dict structure\n",
    "    return list(map(len, h_elements.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807dc169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
