{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7d0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp page_seo_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfdb5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sqlmodel import Session, select\n",
    "from seo_rat.sqlite_db import SQLiteDB\n",
    "from seo_rat.gsc_client import GSCAuth, get_date_range\n",
    "from seo_rat.gsc_storage import get_top_queries, get_page_analytics, get_trends\n",
    "from seo_rat.gsc_insights import (\n",
    "    detect_query_trends,\n",
    "    classify_page_intents,\n",
    "    find_missing_queries,\n",
    "    find_green_keywords,\n",
    "    get_date_ranges_for_comparison,\n",
    ")\n",
    "from seo_rat.content_parser import (\n",
    "    get_page_content,\n",
    "    parse_metadata,\n",
    "    parse_notebook_metadata,\n",
    "    check_title_length,\n",
    "    check_desc_length,\n",
    "    check_content_length,\n",
    "    extract_headers,\n",
    "    extract_images,\n",
    "    extract_links,\n",
    "    imgs_missing_alts,\n",
    ")\n",
    "from seo_rat.seo_content_analysis import check_h1_count, calculate_keyword_density\n",
    "from seo_rat.content_mapper import map_all_urls_to_files\n",
    "from seo_rat.index_tracking import fetch_sitemap_urls\n",
    "from seo_rat.seo_report import analyze_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60450d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "def full_page_audit(\n",
    "    session: Session,\n",
    "    site_url: str,\n",
    "    page_url: str,\n",
    "    file_path: str,\n",
    "    domain: str,\n",
    "    is_quarto: bool = True,\n",
    "    days: int = 90,\n",
    "    query_limit: int = 100,\n",
    ") -> dict:\n",
    "    \"\"\"Generate a comprehensive SEO audit for a single page\"\"\"\n",
    "\n",
    "    # --- Content Analysis ---\n",
    "    content = get_page_content(file_path, is_quarto=is_quarto)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    if file_path.endswith(\".ipynb\"):\n",
    "        metadata = parse_notebook_metadata(raw)\n",
    "    else:\n",
    "        metadata = parse_metadata(raw)\n",
    "\n",
    "    title = metadata.get(\"title\", \"\") or metadata.get(\"pagetitle\", \"\")\n",
    "    description = metadata.get(\"description\", \"\")\n",
    "\n",
    "    headers = extract_headers(file_path)\n",
    "    images = extract_images(content)\n",
    "\n",
    "    basic_seo = {\n",
    "        \"title\": title,\n",
    "        \"title_check\": check_title_length(title),\n",
    "        \"description\": description,\n",
    "        \"description_check\": check_desc_length(description),\n",
    "        \"content_check\": check_content_length(content),\n",
    "        \"h1_check\": check_h1_count(headers, title=title, is_quarto=is_quarto),\n",
    "        \"missing_alts\": imgs_missing_alts(images),\n",
    "        \"link_analysis\": analyze_links(content, domain),\n",
    "    }\n",
    "\n",
    "    # --- GSC Performance ---\n",
    "    start, end = get_date_range(\"last_days\", days=days)\n",
    "    page_path = page_url.split(domain)[-1] if domain in page_url else page_url\n",
    "\n",
    "    gsc_performance = get_page_analytics(session, site_url, page_path, start, end)\n",
    "\n",
    "    # --- Query Intent ---\n",
    "    query_intents = classify_page_intents(\n",
    "        session, site_url, start, end, page_path=page_path, limit=query_limit,\n",
    "    )\n",
    "\n",
    "    # --- Trends ---\n",
    "    trends = detect_query_trends(\n",
    "        session, site_url, page_path=page_path, days=days, limit=query_limit,\n",
    "    )\n",
    "    rising = [t for t in trends if t[\"trend\"] == \"rising\"]\n",
    "    declining = [t for t in trends if t[\"trend\"] == \"declining\"]\n",
    "\n",
    "    # --- Missing Queries ---\n",
    "    queries = get_top_queries(\n",
    "        session, site_url, start, end, page_path=page_path, limit=query_limit,\n",
    "    )\n",
    "    missing = find_missing_queries(queries, content)\n",
    "\n",
    "    # --- Green Keywords ---\n",
    "    green = find_green_keywords(\n",
    "        session, site_url, page_path=page_path, content=content, days=days, limit=query_limit,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"page_url\": page_url,\n",
    "        \"file_path\": file_path,\n",
    "        \"basic_seo\": basic_seo,\n",
    "        \"gsc_performance\": gsc_performance,\n",
    "        \"query_intents\": query_intents,\n",
    "        \"trends\": {\"rising\": rising, \"declining\": declining},\n",
    "        \"missing_queries\": missing,\n",
    "        \"green_keywords\": green,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a4ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "def print_audit(audit: dict):\n",
    "    \"\"\"Pretty print a page audit\"\"\"\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"PAGE AUDIT: {audit['page_url']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Basic SEO\n",
    "    seo = audit[\"basic_seo\"]\n",
    "    print(f\"\\n--- Basic SEO ---\")\n",
    "    print(f\"Title: {seo['title']} ({seo['title_check']['length']} chars, optimal: {seo['title_check']['optimal_length']})\")\n",
    "    print(f\"Description: {seo['description'][:60]}... ({seo['description_check']['length']} chars, optimal: {seo['description_check']['optimal_length']})\")\n",
    "    print(f\"Content: {seo['content_check']['word_count']} words (sufficient: {seo['content_check']['is_sufficient']})\")\n",
    "    print(f\"H1: {seo['h1_check']}\")\n",
    "    print(f\"Missing alts: {seo['missing_alts']}\")\n",
    "    print(f\"Links: {seo['link_analysis']['internal_count']} internal, {seo['link_analysis']['external_count']} external\")\n",
    "\n",
    "    # GSC Performance\n",
    "    perf = audit[\"gsc_performance\"]\n",
    "    print(f\"\\n--- GSC Performance ---\")\n",
    "    print(f\"Clicks: {perf['total_clicks']} | Impressions: {perf['total_impressions']}\")\n",
    "    print(f\"Avg Position: {perf['avg_position']:.1f} | Avg CTR: {perf['avg_ctr']:.2%}\")\n",
    "\n",
    "    # Intent Breakdown\n",
    "    intents = audit[\"query_intents\"]\n",
    "    if intents:\n",
    "        from collections import Counter\n",
    "        intent_counts = Counter(q[\"intent\"] for q in intents)\n",
    "        for intent, count in intent_counts.most_common():\n",
    "            print(f\"\\n  {intent} ({count}):\")\n",
    "            for q in intents:\n",
    "                if q[\"intent\"] == intent:\n",
    "                    print(f\"    {q['query'][:45]:45} | imp: {q['total_impressions']} | clicks: {q['total_clicks']} | pos: {q['avg_position']:.1f}\")\n",
    "\n",
    "\n",
    "    # Trends\n",
    "    rising = audit[\"trends\"][\"rising\"]\n",
    "    declining = audit[\"trends\"][\"declining\"]\n",
    "    print(f\"\\n--- Trends ---\")\n",
    "    print(f\"Rising: {len(rising)} | Declining: {len(declining)}\")\n",
    "    for t in rising[:5]:\n",
    "        print(f\"  â†‘ {t['query'][:45]:45} | imp: {t['impression_change']:+d}\")\n",
    "    for t in declining[:5]:\n",
    "        print(f\"  â†“ {t['query'][:45]:45} | imp: {t['impression_change']:+d}\")\n",
    "\n",
    "    # Missing Queries\n",
    "    missing = audit[\"missing_queries\"]\n",
    "    print(f\"\\n--- Missing Queries ({len(missing)}) ---\")\n",
    "    for m in missing[:10]:\n",
    "        print(f\"  {m['query'][:45]:45} | imp: {m['total_impressions']} | clicks: {m['total_clicks']} | pos: {m['avg_position']:.1f}\")\n",
    "\n",
    "\n",
    "    # Green Keywords\n",
    "    green = audit[\"green_keywords\"]\n",
    "    print(f\"\\n--- Green Keywords ({len(green)}) ---\")\n",
    "    for g in green[:10]:\n",
    "        print(f\"  ğŸŸ¢ {g['query'][:45]:45} | {g['prev_impressions']} â†’ {g['recent_impressions']} | pos: {g['recent_position']:.1f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2bed8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| hide\n",
    "db = SQLiteDB()\n",
    "session = db.get_session()\n",
    "auth = GSCAuth()\n",
    "\n",
    "urls = fetch_sitemap_urls(\"https://kareemai.com/sitemap.xml\")\n",
    "url_mapping = map_all_urls_to_files(\n",
    "    \"/home/kobo/Desktop/obsidian_valuts/logseq/karem-site/\",\n",
    "    \"https://kareemai.com/\",\n",
    "    urls,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad9c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PAGE AUDIT: https://kareemai.com/blog/posts/life style/fake graviety.html\n",
      "======================================================================\n",
      "\n",
      "--- Basic SEO ---\n",
      "Title: Ø¬Ø§Ø°Ø¨ÙŠØ© Ù…Ø²ÙŠÙØ© Ùˆ Ø¹Ø§Ù„Ù… Ù…Ø®Ø§Ø¯Ø¹: ÙƒÙŠÙ ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø¨ÙˆØµÙ„ØªÙƒØŸ (48 chars, optimal: False)\n",
      "Description: Ù…Ø®Ø§ÙˆÙ Ø¹Ù† Ø§Ù„ÙˆÙ‚ÙˆØ¹ ÙÙŠ Ø¬Ø§Ø°Ø¨ÙŠØ© Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† ÙˆØ£Ù‡ÙˆØ§Ø¦Ù‡Ù… ÙˆØ§Ù„Ø§Ø¨ØªØ¹Ø§Ø¯ Ø¹Ù† Ø§Ù„Ù‡Ø¯... (129 chars, optimal: False)\n",
      "Content: 2291 words (sufficient: True)\n",
      "H1: {'h1_count': 1, 'has_single_h1': True, 'h1_source': 'title'}\n",
      "Missing alts: []\n",
      "Links: 4 internal, 1 external\n",
      "\n",
      "--- GSC Performance ---\n",
      "Clicks: 0 | Impressions: 0\n",
      "Avg Position: 0.0 | Avg CTR: 0.00%\n",
      "\n",
      "--- Trends ---\n",
      "Rising: 0 | Declining: 0\n",
      "\n",
      "--- Missing Queries (0) ---\n",
      "\n",
      "--- Green Keywords (0) ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#| hide\n",
    "from urllib.parse import unquote\n",
    "from seo_rat.gsc_storage import normalize_url\n",
    "\n",
    "page_url = normalize_url(\"https://kareemai.com/blog/posts/life_style/fake_graviety.html\")\n",
    "\n",
    "\n",
    "file_path = url_mapping[page_url]\n",
    "\n",
    "audit = full_page_audit(\n",
    "    session,\n",
    "    site_url=\"sc-domain:kareemai.com\",\n",
    "    page_url=page_url,\n",
    "    file_path=file_path,\n",
    "    domain=\"kareemai.com\",\n",
    ")\n",
    "print_audit(audit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af2f0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
