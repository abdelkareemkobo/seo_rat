# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_gsc_storage.ipynb.

# %% auto #0
__all__ = ['filter_site', 'filter_dates', 'filter_dimension', 'AnalyticsSummary', 'normalize_url', 'parse_gsc_row',
           'store_gsc_data', 'get_top_queries', 'filter_exclude_pages', 'get_top_queries_excluding_pages',
           'get_page_analytics', 'get_analytics_by_date_range', 'get_trends', 'get_analytics_by', 'store_single_date',
           'store_date_range']

# %% ../nbs/05_gsc_storage.ipynb #9ba81a16
from sqlmodel import Session, select
from .models import GSCAnalytics
from datetime import datetime,timedelta
from .gsc_client import GSCAuth
from functools import partial
from fastcore.basics import compose
from sqlalchemy import func, not_, or_


# %% ../nbs/05_gsc_storage.ipynb #e3c41198
def filter_site(query, site_url: str):
    return query.where(GSCAnalytics.site_url == site_url)


def filter_dates(query, start: str, end: str):
    return query.where(GSCAnalytics.date.between(start, end))


# %% ../nbs/05_gsc_storage.ipynb #ab8ce74e
def filter_dimension(query, dimension: str, value: str):
    return query.where(getattr(GSCAnalytics, dimension) == value)


# %% ../nbs/05_gsc_storage.ipynb #f5c2c658
from sqlmodel import SQLModel


class AnalyticsSummary(SQLModel):
    query: str
    clicks: int
    impressions: int

# %% ../nbs/05_gsc_storage.ipynb #bf4b349f
from urllib.parse import unquote

def normalize_url(url: str) -> str:
    """Normalize URL by decoding percent-encoding and standardizing separators"""
    url = unquote(url)
    url = url.replace("-", " ").replace("_", " ")
    return url


# %% ../nbs/05_gsc_storage.ipynb #13abf339
def parse_gsc_row(row, site_url, date) -> GSCAnalytics:
    keys = row["keys"]
    fields = ("query", "page", "country", "device")
    dims = dict(zip(fields, keys))
    if dims.get("page"):
        dims["page"] = normalize_url(dims["page"])

    return GSCAnalytics(
        site_url=site_url,
        date=date,
        query=dims.get("query"),
        page=dims.get("page"),
        country=dims.get("country"),
        device=dims.get("device"),
        clicks=row["clicks"],
        impressions=row["impressions"],
        ctr=row["ctr"],
        position=row["position"],
    )

# %% ../nbs/05_gsc_storage.ipynb #b82b12c4
def store_gsc_data(session: Session, site_url: str, date: str, rows: list[dict]):
    """Store GSC data with upsert (update or insert)"""
    for row in rows:
        record = parse_gsc_row(row, site_url, date)
        existing = session.exec(
            select(GSCAnalytics).where(
                GSCAnalytics.site_url == record.site_url,
                GSCAnalytics.date == record.date,
                GSCAnalytics.query == record.query,
                GSCAnalytics.page == record.page,
                GSCAnalytics.country == record.country,
                GSCAnalytics.device == record.device,
            )
        ).first()
        if existing:
            record.id = existing.id
        session.merge(record)
    session.commit()


# %% ../nbs/05_gsc_storage.ipynb #cf40d14b
def get_top_queries(
    session: Session,
    site_url: str,
    start_date: str,
    end_date: str,
    country: str | None = None,
    page_path: str | None = None,
    limit: int = 10,
) -> list[dict]:
    """Get top performing queries, optionally filtered by page"""
    base_query = select(
        GSCAnalytics.query,
        func.sum(GSCAnalytics.clicks).label("total_clicks"),
        func.sum(GSCAnalytics.impressions).label("total_impressions"),
        func.avg(GSCAnalytics.position).label("avg_position"),
        func.avg(GSCAnalytics.ctr).label("avg_ctr"),
    )
    filters = [
        partial(filter_site, site_url=site_url),
        partial(filter_dates, start=start_date, end=end_date),
    ]
    if country:
        filters.append(partial(filter_dimension, dimension="country", value=country))
    if page_path:
        filters.append(lambda q: q.where(GSCAnalytics.page.contains(page_path)))

    query = (compose(*filters)(base_query)
        .where(GSCAnalytics.query.isnot(None))
        .group_by(GSCAnalytics.query)
        .order_by(func.sum(GSCAnalytics.clicks).desc())
        .limit(limit))

    return [row._asdict() for row in session.exec(query)]


# %% ../nbs/05_gsc_storage.ipynb #50fd2ca2
def filter_exclude_pages(query, exclude_pages: list[str]):
    filters = [GSCAnalytics.page.contains(page) for page in exclude_pages]
    return query.where(not_(or_(*filters)))


# %% ../nbs/05_gsc_storage.ipynb #bac24352
def get_top_queries_excluding_pages(
    session: Session,
    site_url: str,
    start_date: str,
    end_date: str,
    exclude_pages: list[str],
    country: str | None = None,
    limit: int = 10,
) -> list[dict]:
    """Get top queries excluding certain pages"""
    base_query = select(
        GSCAnalytics.query,
        func.sum(GSCAnalytics.clicks).label("total_clicks"),
        func.sum(GSCAnalytics.impressions).label("total_impressions"),
    )
    filters = [
        partial(filter_site, site_url=site_url),
        partial(filter_dates, start=start_date, end=end_date),
        partial(filter_exclude_pages, exclude_pages=exclude_pages),
    ]
    if country:
        filters.append(partial(filter_dimension, dimension="country", value=country))

    query = (
        compose(*filters)(base_query)
        .where(GSCAnalytics.query.isnot(None))
        .group_by(GSCAnalytics.query)
        .order_by(func.sum(GSCAnalytics.clicks).desc())
        .limit(limit)
    )

    return [row._asdict() for row in session.exec(query)]


# %% ../nbs/05_gsc_storage.ipynb #437a9e6a
def get_page_analytics(
    session: Session, site_url: str, page_path: str, start_date: str, end_date: str
) -> dict:
    """Get analytics for a specific page"""
    base_query = select(
        func.sum(GSCAnalytics.clicks).label("total_clicks"),
        func.sum(GSCAnalytics.impressions).label("total_impressions"),
        func.avg(GSCAnalytics.position).label("avg_position"),
        func.avg(GSCAnalytics.ctr).label("avg_ctr"),
    )
    filters = compose(
        partial(filter_site, site_url=site_url),
        partial(filter_dates, start=start_date, end=end_date),
    )
    query = filters(base_query).where(GSCAnalytics.page.contains(page_path))

    row = session.exec(query).first()

    # Top queries need a separate query
    q_query = filters(select(GSCAnalytics.query)).where(
        GSCAnalytics.page.contains(page_path),
        GSCAnalytics.query.isnot(None),
    )
    top_queries = list(set(session.exec(q_query)))[:10]

    return {
        "page_path": page_path,
        "total_clicks": row.total_clicks or 0,
        "total_impressions": row.total_impressions or 0,
        "avg_position": row.avg_position or 0,
        "avg_ctr": row.avg_ctr or 0,
        "top_queries": top_queries,
    }


# %% ../nbs/05_gsc_storage.ipynb #cd437b68
def get_analytics_by_date_range(
    session: Session, site_url: str, start_date: str, end_date: str
) -> list[GSCAnalytics]:
    """Get all analytics for date range"""
    base_query = select(GSCAnalytics)
    build_query = compose(
        partial(filter_site, site_url=site_url),
        partial(filter_dates, start=start_date, end=end_date),
    )
    query = build_query(base_query)

    return session.exec(query).all()


# %% ../nbs/05_gsc_storage.ipynb #1d977048
def get_trends(
    session: Session,
    site_url: str,
    start_date: str,
    end_date: str,
    dimension: str | None = None,
) -> list[dict]:
    """Get trends over time, optionally grouped by dimension"""
    columns = [
        GSCAnalytics.date,
        func.sum(GSCAnalytics.clicks).label("clicks"),
        func.sum(GSCAnalytics.impressions).label("impressions"),
        func.avg(GSCAnalytics.position).label("avg_position"),
        func.avg(GSCAnalytics.ctr).label("avg_ctr"),
    ]
    group_by = [GSCAnalytics.date]

    if dimension:
        dim_col = getattr(GSCAnalytics, dimension)
        columns.insert(1, dim_col)
        group_by.append(dim_col)

    base_query = select(*columns)
    filters = compose(
        partial(filter_site, site_url=site_url),
        partial(filter_dates, start=start_date, end=end_date),
    )
    query = filters(base_query).group_by(*group_by).order_by(GSCAnalytics.date)

    return [row._asdict() for row in session.exec(query)]


# %% ../nbs/05_gsc_storage.ipynb #2b0fe945
def get_analytics_by(
    session: Session,
    site_url: str,
    start_date: str,
    end_date: str,
    dimension: str,
    value: str,
) -> list[AnalyticsSummary]:

    base_query = select(
        GSCAnalytics.query,
        func.sum(GSCAnalytics.clicks).label("clicks"),
        func.sum(GSCAnalytics.impressions).label("impressions"),
    )

    build_query = compose(
        partial(filter_site, site_url=site_url),
        partial(filter_dates, start=start_date, end=end_date),
        partial(filter_dimension, dimension=dimension, value=value),
    )
    return [
        AnalyticsSummary(**row._asdict())
        for row in session.exec(build_query(base_query).group_by(GSCAnalytics.query))
    ]


# %% ../nbs/05_gsc_storage.ipynb #c41b1113
from .gsc_client import fetch_gsc_data
import time

def store_single_date(session: Session, auth: GSCAuth, site_url: str, date: str) -> int:
    """Store GSC data for a single date"""

    rows = fetch_gsc_data(auth, site_url, date, date)
    store_gsc_data(session, site_url, date, rows)
    return len(rows)


# %% ../nbs/05_gsc_storage.ipynb #3105675a
def store_date_range(
    session: Session, auth: GSCAuth, site_url: str, start_date: str, end_date: str
) -> dict:
    """Store GSC data for a date range with progress"""
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    total_days = (end - start).days + 1

    results = {"successful": [], "failed": [], "total_records": 0}

    for day_num in range(total_days):
        date_str = (start + timedelta(days=day_num)).strftime("%Y-%m-%d")
        print(f"Processing {date_str} ({day_num + 1}/{total_days})...")

        try:
            count = store_single_date(session, auth, site_url, date_str)
            results["successful"].append(date_str)
            results["total_records"] += count
        except Exception as e:
            results["failed"].append({"date": date_str, "error": str(e)})

        time.sleep(1)

    return results


