# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08_semantic_search.ipynb.

# %% auto 0
__all__ = ['KeywordVectorDB', 'store_page_keywords']

# %% ../nbs/08_semantic_search.ipynb 1
from .indextime import  get_file_names

get_file_names()

# %% ../nbs/08_semantic_search.ipynb 4
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, PointStruct, Distance
from datetime import datetime
from typing import Dict, List, Optional
from fastapi import HTTPException
from uuid import uuid4

class KeywordVectorDB:
    def __init__(self):
        self.model = SentenceTransformer(
            "Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2",device="cuda"
        )
        self.client = QdrantClient("localhost", port=6333)

    def get_or_create_collection(self, page_url: str) -> str:
        """Get existing collection or create if doesn't exist"""
        collection_name = self._get_collection_name(page_url)

        # Check if collection exists
        collections = self.client.get_collections().collections
        exists = any(c.name == collection_name for c in collections)

        if not exists:
            self.client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE),
            )

        return collection_name

    def process_search_analytics(self, analytics_data: Dict):
        """Process search analytics data and update/store in vector DB"""
        collection_name = self.get_or_create_collection(analytics_data["page_url"])
        points_to_upsert = []  # Collect all points here

        for query_data in analytics_data["query_performance"]:
            keyword = query_data["_id"]
            keyword_vector = self.model.encode(keyword)
            point_id = str(uuid4())

            # Check if keyword exists
            search_result = self.client.search(
                collection_name=collection_name,
                query_vector=keyword_vector,
                limit=1,
                score_threshold=0.95,
            )

            # Prepare new metadata
            new_metadata = {
                "keyword": keyword,
                "site_url": analytics_data["site_url"],
                "page_url": analytics_data["page_url"],
                "date_range": analytics_data["date_range"],
                "performance": {
                    "clicks": query_data["clicks"],
                    "impressions": query_data["impressions"],
                    "position": query_data["avg_position"],
                    "ctr": query_data["ctr"],
                    "dates": query_data["dates"],
                },
                "last_updated": datetime.now().isoformat(),
            }

            if search_result:
                # Update existing keyword
                existing_point = search_result[0]
                point_id = existing_point.id

                # Preserve existing metadata fields
                existing_metadata = existing_point.payload
                new_metadata["in_content"] = existing_metadata.get("in_content", False)

                new_metadata['is_optimized'] = existing_metadata.get("is_optimized",False)
                new_metadata["is_important"] = existing_metadata.get("is_important",False)
                new_metadata["is_focus_keyword"] = existing_metadata.get(
                    "is_focus_keyword", False
                )

            # Create point structure
            point = PointStruct(
                id=point_id,
                vector=keyword_vector.tolist(),
                payload=new_metadata,
            )
            points_to_upsert.append(point)

        # Single batch upsert for all points
        if points_to_upsert:
            self.client.upsert(collection_name=collection_name, points=points_to_upsert)

    def _get_collection_name(self, url: str) -> str:
        """Convert URL to valid collection name"""
        return url.replace("https://", "").replace("/", "_").replace(".", "_")
    def search_keywords(
        self,
        page_url: str,
        query_text: str = None,
        limit: int = 10,
        filter_params: Dict = None,
    ) -> List[Dict]:
        """Search keywords in a page collection"""
        collection_name = self._get_collection_name(page_url)

        if query_text:
            # Vector search with query
            query_vector = self.model.encode(query_text)
            results = self.client.search(
                collection_name=collection_name,
                query_vector=query_vector.tolist(),
                query_filter=filter_params,
                limit=limit,
            )
        else:
            # Regular filtering without vector search
            results = self.client.scroll(
                collection_name=collection_name, scroll_filter=filter_params, limit=limit
            )[0]  # scroll returns tuple (points, offset)

        # Format results
        return [
            {
                "keyword": point.payload["keyword"],
                "score": getattr(point, "score", 1.0),
                **point.payload,
            }
            for point in results
        ]

    def get_page_keywords(
        self,
        page_url: str,
        category: str = None,
        in_content: bool = None,
        min_clicks: int = None,
    ) -> List[Dict]:
        """Get keywords for a specific page with optional filters"""

        conditions = []

        if category:
            conditions.append({"key": "category", "match": {"value": category}})
        if in_content is not None:
            conditions.append({"key": "in_content", "match": {"value": in_content}})
        if min_clicks:
            conditions.append({"key": "performance.clicks", "range": {"gte": min_clicks}})

        filter_params = {"must": conditions} if conditions else None

        return self.search_keywords(page_url, filter_params=filter_params)
    def update_content_status(self, page_url: str, markdown_content: str):
        """Update in_content status for all keywords based on markdown content"""
        collection_name = self._get_collection_name(page_url)
        results = self.client.scroll(collection_name=collection_name, limit=1000)[0]

        for point in results:
            metadata = point.payload
            keyword = metadata["keyword"]

            # Update just the metadata
            metadata["in_content"] = keyword.lower() in markdown_content.lower()
            metadata["last_updated"] = datetime.now().isoformat()


            # Update only the payload for this point ID
            self.client.set_payload(
                collection_name=collection_name, payload=metadata, points=[point.id]
            )
    def set_keyword_importance(self, page_url: str, keyword: str, is_important: bool):
        """Set importance status for a specific keyword"""
        collection_name = self._get_collection_name(page_url)

        # Find the keyword
        results = self.client.scroll(
            collection_name=collection_name,
            filter={"must": [{"key": "keyword", "match": {"value": keyword}}]},
            limit=1,
        )[0]

        if results:
            point = results[0]
            metadata = point.payload
            metadata["is_important"] = is_important
            metadata["last_updated"] = datetime.now().isoformat()

            self.client.set_payload(
                collection_name=collection_name, payload=metadata, points=[point.id]
            )
            return True
        return False


    def get_keyword_importance(self, page_url: str, keyword: str) -> Optional[bool]:
        """Get importance status for a specific keyword"""
        collection_name = self._get_collection_name(page_url)

        results = self.client.scroll(
            collection_name=collection_name,
            filter={"must": [{"key": "keyword", "match": {"value": keyword}}]},
            limit=1,
        )[0]

        if results:
            return results[0].payload.get("is_important", False)
        return None


# %% ../nbs/08_semantic_search.ipynb 5
async def store_page_keywords(
    site_url: str, page_url: str, range_type: str = "last_days", days: int = 7
):
    """Store page keywords in vector database"""
    try:
        # Get search analytics data
        analytics_data = await search_page_analytics(
            site_url=site_url,
            page_url=page_url,
            range_type=range_type,
            days=days,
            show_queries=True,
        )

        # Initialize vector DB
        vector_db = KeywordVectorDB()

        # Process and store data
        vector_db.process_search_analytics(analytics_data)

        return {
            "status": "success",
            "message": f"Stored keywords for {page_url}",
            "collection_name": vector_db._get_collection_name(page_url),
        }

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to store keywords: {str(e)}"
        )

