# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_seo_report.ipynb.

# %% auto #0
__all__ = ['sync_articles_to_db', 'find_duplicate_metadata', 'analyze_links', 'generate_seo_report']

# %% ../nbs/09_seo_report.ipynb #55e2ad2b
from typing import Dict, List
from sqlmodel import Session, select
from .article import get_article_by_path, insert_article, Article
from .sqlite_db import SQLiteDB
from .models import Website
from .content_mapper import map_all_urls_to_files
from seo_rat.content_parser import (
    parse_notebook_metadata,
    filter_external_links,
    filter_internal_links,
    extract_links,
)


# %% ../nbs/09_seo_report.ipynb #fb2feeae
def sync_articles_to_db(
    session: Session, website_id: int, url_file_mapping: Dict[str, str]
):
    """Insert all articles into database"""
    for url, file_path in url_file_mapping.items():
        if file_path:  # Only if file exists
            # Check if already exists
            existing = get_article_by_path(session, file_path)
            if not existing:
                insert_article(session, website_id, file_path)


# %% ../nbs/09_seo_report.ipynb #24bd6d69
def find_duplicate_metadata(
    session: Session, field: str, similarity_threshold: float = 0.9
) -> List[Dict]:
    """Find pages with duplicate or very similar titles"""
    from seo_rat.content_parser import parse_metadata, calculate_similarity

    articles = session.exec(select(Article)).all()
    titles = {}

    # Collect all titles
    for article in articles:
        with open(article.file_path, "r") as f:
            if article.file_path.endswith(".ipynb"):
                metdata = parse_notebook_metadata(f.read())
                titles[article.file_path] = metdata.get(field, "")
            elif article.file_path.endswith(".md") or article.file_path.endswith(
                ".qmd"
            ):
                metadata = parse_metadata(f.read())
                titles[article.file_path] = metadata.get(field, "")

    # Find duplicates
    duplicates = []
    checked = set()

    for path1, field1 in titles.items():
        for path2, field2 in titles.items():
            if path1 >= path2 or (path1, path2) in checked:
                continue

            similarity = calculate_similarity(field1, field2)
            if similarity >= similarity_threshold:
                duplicates.append(
                    {
                        "file1": path1,
                        "file2": path2,
                        f"{field}{1}": field1,
                        f"{field}{2}": field2,
                        "similarity": similarity,
                    }
                )
            checked.add((path1, path2))

    return duplicates


# %% ../nbs/09_seo_report.ipynb #69064449
def analyze_links(content: str, domain: str) -> Dict:
    """Analyze internal and external links in content"""
    links = extract_links(content)
    all_urls = list(links.keys())

    internal = filter_internal_links(all_urls, domain)
    external = filter_external_links(all_urls, domain)

    return {
        "total_links": len(all_urls),
        "internal_count": len(internal),
        "external_count": len(external),
        "internal_links": internal,
        "external_links": external,
    }


# %% ../nbs/09_seo_report.ipynb #a1e0e1e8
def generate_seo_report(session: Session, website_id: int, domain: str) -> Dict:
    """Generate comprehensive SEO report for all pages"""

    articles = session.exec(
        select(Article).where(Article.website_id == website_id)
    ).all()

    page_reports = []
    issues = []

    for article in articles:
        try:
            with open(article.file_path, "r") as f:
                raw_content = f.read()

            # Parse metadata
            if article.file_path.endswith(".ipynb"):
                metadata = parse_notebook_metadata(raw_content)
            else:
                metadata = parse_metadata(raw_content)

            content = remove_metadata(raw_content)
            headers = extract_headers(article.file_path)
            images = extract_images(content)

            # Run checks
            report = {
                "file_path": article.file_path,
                "title_check": check_title_length(metadata),
                "description_check": check_desc_length(metadata),
                "content_check": check_content_length(content),
                "h1_check": check_h1_count(
                    headers, title=metadata["title"], is_quarto=True
                ),
                "missing_alts": imgs_missing_alts(images),
                "link_analysis": analyze_links(content, domain),
            }

            # Collect issues
            if not report["h1_check"]["has_single_h1"]:
                issues.append(f"{article.file_path}: Multiple or no H1")
            if not report["content_check"]["is_sufficient"]:
                issues.append(f"{article.file_path}: Content too short")

            page_reports.append(report)

        except Exception as e:
            print(f"Error analyzing {article.file_path}: {e}")

    # Site-wide checks
    duplicate_titles = find_duplicate_metadata(session, "title", 0.9)
    duplicate_descriptions = find_duplicate_metadata(session, "description", 0.9)

    return {
        "total_pages": len(articles),
        "pages_analyzed": len(page_reports),
        "page_reports": page_reports,
        "duplicate_titles": duplicate_titles,
        "duplicate_descriptions": duplicate_descriptions,
        "issues": issues,
        "summary": {
            "total_issues": len(issues),
            "duplicate_titles_count": len(duplicate_titles),
            "duplicate_descriptions_count": len(duplicate_descriptions),
        },
    }

