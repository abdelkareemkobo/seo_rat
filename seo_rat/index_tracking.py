# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_index_tracking.ipynb.

# %% auto #0
__all__ = ['inspect_url_status', 'store_index_status', 'get_index_status', 'get_not_indexed_pages', 'fetch_sitemap_urls',
           'store_all_index_status']

# %% ../nbs/06_index_tracking.ipynb #b28f692b
from sqlmodel import Session, select
from .models import IndexStatus
from .gsc_client import GSCAuth
from googleapiclient.discovery import build
from typing import List, Optional,Dict
from datetime import datetime


# %% ../nbs/06_index_tracking.ipynb #db5cae1f
def inspect_url_status(auth: GSCAuth, site_url: str, page_url: str) -> dict:
    """Inspect URL indexing status from GSC"""
    service = build("searchconsole", "v1", credentials=auth.get_credentials())

    request = {"inspectionUrl": page_url, "siteUrl": site_url, "languageCode": "en-US"}

    response = service.urlInspection().index().inspect(body=request).execute()
    result = response.get("inspectionResult", {}).get("indexStatusResult", {})

    return {
        "verdict": result.get("verdict", "UNKNOWN"),
        "coverage_state": result.get("coverageState"),
        "last_crawl_time": result.get("lastCrawlTime"),
        "indexing_state": result.get("indexingState"),
        "robots_txt_state": result.get("robotsTxtState"),
    }


# %% ../nbs/06_index_tracking.ipynb #4e7493ca
def store_index_status(session: Session, auth: GSCAuth, site_url: str, page_url: str):
    """Inspect and store URL index status"""
    status_data = inspect_url_status(auth, site_url, page_url)

    # Upsert
    existing = session.exec(
        select(IndexStatus).where(IndexStatus.page_url == page_url)
    ).first()

    if existing:
        for key, value in status_data.items():
            setattr(existing, key, value)
        existing.checked_at = datetime.now()
    else:
        index_status = IndexStatus(site_url=site_url, page_url=page_url, **status_data)
        session.add(index_status)

    session.commit()


# %% ../nbs/06_index_tracking.ipynb #e7b6358d
def get_index_status(
    session: Session, site_url: str, verdict: str = None
) -> List[IndexStatus]:
    """Get stored index status for a site"""
    stmt = select(IndexStatus).where(IndexStatus.site_url == site_url)
    if verdict:
        stmt = stmt.where(IndexStatus.verdict == verdict)
    return session.exec(stmt).all()


# %% ../nbs/06_index_tracking.ipynb #14a7b023
def get_not_indexed_pages(session: Session, site_url: str) -> List[IndexStatus]:
    """Get pages that are not indexed"""
    return session.exec(
        select(IndexStatus).where(
            IndexStatus.site_url == site_url, IndexStatus.verdict != "PASS"
        )
    ).all()


# %% ../nbs/06_index_tracking.ipynb #73794973
def fetch_sitemap_urls(sitemap_url: str) -> List[str]:
    """Fetch all URLs from sitemap XML"""
    import httpx
    from bs4 import BeautifulSoup

    response = httpx.get(sitemap_url)
    soup = BeautifulSoup(response.text, "xml")
    return [loc.text for loc in soup.find_all("loc")]


# %% ../nbs/06_index_tracking.ipynb #f514af6b
def store_all_index_status(
    session: Session, auth: GSCAuth, site_url: str, sitemap_url: str
):
    """Check and store index status for all pages in sitemap"""
    urls = fetch_sitemap_urls(sitemap_url)
    total = len(urls)

    for i, url in enumerate(urls, 1):
        print(f"Checking {i}/{total}: {url}")
        store_index_status(session, auth, site_url, url)

