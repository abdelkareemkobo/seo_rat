# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_content_parser.ipynb.

# %% auto #0
__all__ = ['parse_metadata', 'parse_notebook_metadata', 'remove_metadata', 'extract_headers', 'check_title_length',
           'check_desc_length', 'check_content_length', 'extract_links', 'extract_images', 'imgs_missing_alts',
           'filter_internal_links', 'filter_external_links', 'normalize_text', 'detect_phone_numbers',
           'calculate_similarity', 'get_file_paths', 'get_file_name', 'get_markdown_files', 'arabic_to_slug',
           'map_files_to_slugs']

# %% ../nbs/02_content_parser.ipynb #49f44ecd
import re
import yaml
from pathlib import Path
from typing import Dict, List, Tuple
from urllib.parse import urlparse
from datetime import datetime

# %% ../nbs/02_content_parser.ipynb #9764a51b
def parse_metadata(content: str) -> Dict:
    """Extract metadata from content frontmatter"""
    yaml_section = content.split("---")[1]
    return yaml.safe_load(yaml_section)


# %% ../nbs/02_content_parser.ipynb #d40b9c62
def parse_notebook_metadata(content: str) -> Dict:
    """Extract metadata from Jupyter notebook"""
    import json

    notebook = json.loads(content)

    # Check first cell for YAML frontmatter
    if notebook.get("cells"):
        first_cell = notebook["cells"][0]
        if first_cell.get("cell_type") == "markdown":
            source = "".join(first_cell.get("source", []))
            if source.startswith("---"):
                return parse_metadata(source)

    return {}


# %% ../nbs/02_content_parser.ipynb #f9460e05
def remove_metadata(content: str) -> str:
    """Remove frontmatter from content"""
    end = content.find("---", 3)
    return content[end + 3 :].strip() if end != -1 else content


# %% ../nbs/02_content_parser.ipynb #657150ca
def extract_headers(file_path: str) -> List[Dict]:
    """Extract all headers with metadata"""
    headings = []
    with open(file_path, "r") as file:
        for line_number, line in enumerate(file, start=1):
            line = line.strip()
            for level in range(1, 7):
                prefix = "#" * level + " "
                if line.startswith(prefix):
                    content = line.strip("#").strip()
                    headings.append(
                        {
                            "type": f"h{level}",
                            "line_number": line_number,
                            "content": content,
                            "length": len(content),
                        }
                    )
                    break
    return headings


# %% ../nbs/02_content_parser.ipynb #09640fda
def check_title_length(title: str) -> Dict:
    length = len(title)
    return {"length": length, "optimal_lenth": 50 <= length <= 60}


# %% ../nbs/02_content_parser.ipynb #4661124e
def check_desc_length(description: str) -> Dict:
    length = len(description)
    return {"length": length, "optimal_lenth": 150 <= length <= 160}


# %% ../nbs/02_content_parser.ipynb #cc169660
def check_content_length(content: str) -> Dict:
    """Count words in content"""
    words = len(content.split())
    return {"word_count": words, "is_sufficient": words >= 300}


# %% ../nbs/02_content_parser.ipynb #cc56687a
def extract_links(content: str) -> Dict[str, Dict]:
    """Extract all links with metadata"""
    links = {}
    lines = content.split("\n")
    for line_number, line in enumerate(lines, start=1):
        for match in re.finditer(r"\[(.*?)\]\((.*?)\)", line):
            title, url = match.groups()
            if url not in links:
                links[url] = {"titles": [], "lines": []}
            links[url]["titles"].append(title)
            links[url]["lines"].append(line_number)
    return links


# %% ../nbs/02_content_parser.ipynb #902e3a86
def extract_images(content: str) -> List[Dict]:
    """Extract images with alt text"""
    matches = re.findall(r"\!\[(.*?)\]\((.*?)\)", content)
    return [{"alt_text": alt, "url": url} for alt, url in matches]


# %% ../nbs/02_content_parser.ipynb #67a91e1b
def imgs_missing_alts(images: List[Dict]) -> List[str]:
    """Return URLs of images missing alt text"""
    return [img["url"] for img in images if not img.get("alt_text")]


# %% ../nbs/02_content_parser.ipynb #92e2970b
def filter_internal_links(urls: List[str], domain: str) -> List[str]:
    """Filter for internal links (excludes images)"""
    image_exts = (".png", ".jpg", ".jpeg", ".gif", ".bmp", ".svg", ".webp")
    internal = []

    for url in urls:
        # Skip images
        if url.lower().endswith(image_exts):
            continue
        # Skip anchors
        if url.startswith("#"):
            continue
        # Relative paths are internal
        if not url.startswith("http"):
            internal.append(url)
        # Same domain
        elif urlparse(url).netloc == domain:
            internal.append(url)

    return internal


# %% ../nbs/02_content_parser.ipynb #d120d701
def filter_external_links(urls: List[str], domain: str) -> List[str]:
    """Filter for external links only"""
    image_exts = (".png", ".jpg", ".jpeg", ".gif", ".bmp", ".svg", ".webp")
    internal = filter_internal_links(urls, domain)

    return [
        url
        for url in urls
        if url not in internal  # Exclude internal
        and not url.lower().endswith(image_exts)
    ]  # Exclude images


# %% ../nbs/02_content_parser.ipynb #67e05771
def normalize_text(text: str) -> str:
    """Normalize text by removing extra whitespace"""
    return re.sub(r"\s+", " ", text).strip()


# %% ../nbs/02_content_parser.ipynb #dd856f20
def detect_phone_numbers(text: str) -> List[str]:
    """Extract phone numbers from text"""
    phone_regex = re.compile(r"(\+\d{1,3})?\s*?(\d{3})\s*?(\d{3})\s*?(\d{3,4})")
    groups = phone_regex.findall(text)
    return ["".join(g) for g in groups]


# %% ../nbs/02_content_parser.ipynb #2b08ed7a
def calculate_similarity(text1: str, text2: str) -> float:
    """Calculate similarity ratio between two texts"""
    from difflib import SequenceMatcher

    return SequenceMatcher(None, text1, text2).ratio()


# %% ../nbs/02_content_parser.ipynb #21534583
def get_file_paths(pattern: str) -> List[str]:
    """Get file paths matching pattern"""
    import glob

    return glob.glob(pattern, recursive=True)


# %% ../nbs/02_content_parser.ipynb #a14b3884
def get_file_name(file_path: str) -> str:
    """Extract filename without extension from path"""
    return Path(file_path).stem


# %% ../nbs/02_content_parser.ipynb #4094fee4
def get_markdown_files(directory: str) -> List[str]:
    """Get all markdown filenames (without extension) from directory"""
    import os

    return [
        f.replace(".md", "")
        for f in os.listdir(directory)
        if f.endswith(".md") and f != ".obsidian"
    ]


# %% ../nbs/02_content_parser.ipynb #b6913bca
def arabic_to_slug(text: str) -> str:
    """Convert Arabic text to URL-friendly slug"""
    char_map = {
        "ا": "a",
        "ب": "b",
        "ت": "t",
        "ث": "th",
        "ج": "j",
        "ح": "h",
        "خ": "kh",
        "د": "d",
        "ذ": "th",
        "ر": "r",
        "ز": "z",
        "س": "s",
        "ش": "sh",
        "ص": "s",
        "ض": "d",
        "ط": "t",
        "ظ": "z",
        "ع": "",
        "غ": "gh",
        "ف": "f",
        "ق": "q",
        "ك": "k",
        "ل": "l",
        "م": "m",
        "ن": "n",
        "ه": "h",
        "و": "w",
        "ي": "y",
        "ة": "h",
        " ": "-",
    }

    slug = "".join(char_map.get(c, c) for c in text.strip().lower())
    while "--" in slug:
        slug = slug.replace("--", "-")
    return slug.strip("-")


# %% ../nbs/02_content_parser.ipynb #0e334816
def map_files_to_slugs(directory: str) -> Dict[str, str]:
    """Map markdown filenames to URL slugs"""
    files = get_markdown_files(directory)
    return {filename: arabic_to_slug(filename) for filename in files}

